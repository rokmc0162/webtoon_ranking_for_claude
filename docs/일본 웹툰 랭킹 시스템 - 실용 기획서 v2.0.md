# 일본 웹툰 랭킹 시스템 - 실용 기획서

**버전**: v2.0
**작성일**: 2026-02-15
**프로젝트명**: JP Webtoon Ranking System
**운영**: RIVERSE Japan 사무실 (24시간 맥북)

---

## 📌 핵심 요약 (30초 요약)

**무엇을**: 일본 4대 웹툰 플랫폼의 일일 랭킹 50위를 자동 수집
**왜**: 리버스 작품 성과 모니터링 + 일본 시장 트렌드 분석 + 경쟁사 추적
**어떻게**: Python 크롤러 (매일 자동 실행) + Streamlit 대시보드
**비용**: $0/년 (무료)
**개발 기간**: 3일
**유지보수**: 월 1시간

---

## 1. 프로젝트 배경

### 1.1 문제 정의
- RIVERSE 작품들이 일본 4대 플랫폼에서 며칠 순위인지 **매일 수동으로 확인 중**
- 경쟁작 동향 파악을 위해 **엑셀로 수작업 관리** → 비효율적
- 일본어 제목만 있어 한국 본사 팀원들이 **작품 파악 어려움**

### 1.2 해결 방안
- **자동 크롤링**: 매일 오전 9시 자동으로 4개 플랫폼 랭킹 수집
- **일일 데이터 누적**: SQLite에 매일 크롤링 결과를 계속 쌓아서 영구 보관
- **순위 변동 그래프**: 특정 작품의 시간별 순위 추이를 라인 차트로 시각화 (핵심 기능!)
- **작품 링크**: 제목 클릭 시 실제 서비스 페이지로 바로 이동
- **대시보드**: 웹 브라우저에서 실시간 조회
- **한국어 매핑**: 일본어 제목 옆에 한국어 제목 자동 표시
- **리버스 강조**: 자사 작품 자동 식별 및 시각적 강조

### 1.3 사용자
- **1차**: 일본 현지 팀 (크롤링 모니터링, 일본어 원어민)
- **2차**: 한국 본사 기획팀 (데이터 분석, 한국어 필요)
- **3차**: 경영진 (월간 리포트용 데이터 활용)

---

## 2. 크롤링 대상

| 플랫폼 | URL | 수집 범위 | 크롤링 방식 | IP 제한 |
|--------|-----|----------|------------|---------|
| **픽코마** | https://piccoma.com/web/ranking/S/P/0 | SMARTOON 종합 1-50위 | SSR (fetch 가능) | 일본 IP 필수 |
| **라인망가** | https://manga.line.me/periodic/gender_ranking?gender=0 | 웹 종합 1-50위 | **CSR (Playwright 필수)** | 일본 IP 필수 |
| **메챠코믹** | https://mechacomic.jp/sales_rankings/current | 판매 종합 1-50위 | CSR (Playwright 필수) | 제한 없음 |
| **코믹시모아** | https://www.cmoa.jp/search/purpose/ranking/all/ | 종합 1-50위 | TLS 이슈 (Playwright 필수) | 제한 없음 |

**총 수집량**: 매일 200개 작품 (4개 플랫폼 × 50위)

**⚠️ 중요**: 라인망가는 JavaScript로 동적 렌더링되므로, 일반 HTTP 요청으로는 크롤링 불가능합니다. 반드시 Playwright 같은 브라우저 자동화 도구를 사용해야 합니다.

---

## 3. 시스템 아키텍처 (단순화)

### 3.1 기술 스택

```
┌─────────────────────────────────────────┐
│  크롤러 (Python)                         │
│  - Playwright (브라우저 크롤링)          │
│  - BeautifulSoup (HTML 파싱)            │
│  - 매일 09:00 JST 자동 실행 (cron)      │
└────────────┬────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────┐
│  데이터 저장 (SQLite)                    │
│  - 로컬 DB 파일 (data/rankings.db)      │
│  - JSON 백업 (data/backup/YYYY-MM-DD/)  │
│  - 영구 보관 (1년 = ~50MB)              │
└────────────┬────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────┐
│  대시보드 (Streamlit)                    │
│  - 브라우저 웹 UI                        │
│  - 날짜/플랫폼별 조회                    │
│  - 순위 변동 그래프                      │
│  - 리버스 작품 하이라이트                │
└─────────────────────────────────────────┘
```

### 3.2 왜 Python + Streamlit?

| 비교 항목 | TypeScript 풀스택 (마누스안) | Python + Streamlit (선택) |
|-----------|----------------------------|---------------------------|
| **코드량** | 6,100줄 | 900줄 (85% 감소) |
| **개발 기간** | 2주 | 3일 |
| **비용** | $660/년 | $0/년 |
| **복잡도** | React + tRPC + Drizzle + Vite | Python 단일 언어 |
| **배포** | 10단계 (launchd + API + 빌드) | cron 1줄 |
| **유지보수** | 여러 기술 이해 필요 | Python만 알면 됨 |
| **대시보드** | 직접 구현 652줄 | Streamlit 150줄 |
| **CSR 크롤링** | Playwright (동일) | Playwright (동일) |

**결론**: 실용성 우선 → Python + Streamlit

**Playwright 사용 이유**: 라인망가/메챠코믹/코믹시모아는 모두 CSR 방식이므로, 브라우저 렌더링 없이는 데이터 수집이 불가능합니다. Python의 Playwright는 TypeScript 버전과 동일한 API를 제공하므로 성능 차이가 없습니다.

---

## 4. 데이터 구조

### 4.1 SQLite 테이블

```sql
CREATE TABLE rankings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date TEXT NOT NULL,           -- 2026-02-15
    platform TEXT NOT NULL,        -- piccoma, linemanga, mechacomic, cmoa
    rank INTEGER NOT NULL,         -- 1-50
    title TEXT NOT NULL,           -- 俺だけレベルアップな件
    title_kr TEXT,                 -- 나 혼자만 레벨업
    genre TEXT,                    -- ファンタジー
    genre_kr TEXT,                 -- 판타지
    url TEXT NOT NULL,             -- https://piccoma.com/web/product/123
    is_riverse BOOLEAN DEFAULT 0,  -- 1 = 리버스 작품
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(date, platform, rank)
);

CREATE INDEX idx_date_platform ON rankings(date, platform);
CREATE INDEX idx_riverse ON rankings(is_riverse) WHERE is_riverse = 1;
CREATE INDEX idx_title ON rankings(title);
```

**수집 데이터 항목**:
- ✅ **랭킹** (1-50위)
- ✅ **제목** (일본어 + 한국어 매핑)
- ✅ **장르** (일본어 + 한국어 번역)
- ✅ **URL** (작품 페이지 링크)
- ❌ ~~출판사, 작가 정보~~ (제외)

### 4.2 참조 데이터

**riverse_titles.json** (407개 작품):
```json
{
  "俺だけレベルアップな件": "나 혼자만 레벨업",
  "婚約破棄された令嬢を拾った俺が": "파혼당한 영애를 주운 나는",
  ...
}
```

**title_mappings.json** (3,135개 작품):
```json
{
  "東京卍リベンジャーズ": "도쿄 리벤저스",
  "転生したらスライムだった件": "전생했더니 슬라임이었던 건에 대하여",
  ...
}
```

---

## 5. 크롤러 구현

### 5.1 디렉토리 구조

```
crawler/
├── main.py                # 크롤러 실행 메인
├── platforms/
│   ├── __init__.py
│   ├── piccoma.py         # 픽코마 크롤러
│   ├── linemanga.py       # 라인망가 크롤러
│   ├── mechacomic.py      # 메챠코믹 크롤러
│   └── cmoa.py            # 코믹시모아 크롤러
├── db.py                  # SQLite 헬퍼
└── utils.py               # 공통 유틸 (제목 매핑, 리버스 판별)
```

### 5.2 플랫폼별 크롤링 전략

각 플랫폼은 웹 기술 스택이 다르므로, 최적화된 방식을 선택해야 합니다:

| 플랫폼 | 렌더링 방식 | 크롤링 엔진 | 주의사항 |
|--------|-----------|-----------|---------|
| 픽코마 | SSR | fetch + BeautifulSoup | HTML에 모든 데이터 포함, 가장 빠름 |
| 라인망가 | **CSR** | **Playwright 필수** | JavaScript 렌더링 없이는 데이터 없음 |
| 메챠코믹 | CSR | Playwright 필수 | 페이지네이션 3페이지 |
| 코믹시모아 | CSR | Playwright 필수 | fetch 시 TLS 에러 |

### 5.3 핵심 로직 예시

```python
# main.py
import asyncio
from datetime import datetime
from playwright.async_api import async_playwright
from platforms import piccoma, linemanga, mechacomic, cmoa
from db import save_rankings, backup_to_json

async def crawl_all():
    """4개 플랫폼 순차 크롤링"""
    date = datetime.now().strftime('%Y-%m-%d')
    print(f"🚀 {date} 크롤링 시작...")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        # 4개 플랫폼 크롤링
        results = {}
        for platform_name, crawler in [
            ('piccoma', piccoma),
            ('linemanga', linemanga),  # ⚠️ CSR, 브라우저 필수
            ('mechacomic', mechacomic),
            ('cmoa', cmoa)
        ]:
            try:
                print(f"  📱 {platform_name} 크롤링 중...")
                rankings = await crawler.crawl(browser)
                results[platform_name] = rankings
                print(f"  ✅ {platform_name}: {len(rankings)}개 작품")
            except Exception as e:
                print(f"  ❌ {platform_name} 실패: {e}")
                results[platform_name] = []

        await browser.close()

    # DB 저장 + JSON 백업
    total_count = 0
    for platform, rankings in results.items():
        save_rankings(date, platform, rankings)
        backup_to_json(date, platform, rankings)
        total_count += len(rankings)

    print(f"✅ 크롤링 완료: {total_count}개 작품 저장")
    return results

if __name__ == "__main__":
    asyncio.run(crawl_all())
```

```python
# platforms/piccoma.py (SSR - 브라우저 불필요, fetch로도 가능)
async def crawl(browser):
    """픽코마 SMARTOON 랭킹 50위 (SSR 페이지)"""
    page = await browser.new_page()
    await page.goto('https://piccoma.com/web/ranking/S/P/0')
    await page.wait_for_selector('.ranking-item', timeout=10000)

    items = await page.query_selector_all('.ranking-item')
    rankings = []

    for i, item in enumerate(items[:50], 1):
        title_elem = await item.query_selector('.title')
        title = await title_elem.inner_text() if title_elem else "알 수 없음"

        link_elem = await item.query_selector('a')
        url = await link_elem.get_attribute('href') if link_elem else ""
        if url and not url.startswith('http'):
            url = f"https://piccoma.com{url}"

        genre_elem = await item.query_selector('.genre')
        genre = await genre_elem.inner_text() if genre_elem else ""

        rankings.append({
            'rank': i,
            'title': title.strip(),
            'url': url,
            'genre': genre.strip()
        })

    await page.close()
    return rankings
```

```python
# platforms/linemanga.py (CSR - Playwright 필수!)
async def crawl(browser):
    """라인망가 웹 종합 랭킹 50위 (CSR 페이지)

    ⚠️ 주의: JavaScript 렌더링 없이는 크롤링 불가능!
    일반 requests/fetch로는 빈 HTML만 받아오므로 반드시 Playwright 사용
    """
    page = await browser.new_page()

    # 페이지 접속
    await page.goto('https://manga.line.me/periodic/gender_ranking?gender=0')

    # JavaScript 렌더링 대기 (중요!)
    await page.wait_for_selector('a[hint]', timeout=15000)

    # 무한 스크롤 처리 (50개 작품 로드)
    for _ in range(10):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(500)

    # 작품 요소 추출
    items = await page.query_selector_all('a[hint]')
    rankings = []

    for i, item in enumerate(items[:50], 1):
        title = await item.get_attribute('hint')
        url = await item.get_attribute('href')
        if url and not url.startswith('http'):
            url = f"https://manga.line.me{url}"

        # 장르는 텍스트에서 추출
        text = await item.inner_text()
        genre = extract_genre_from_text(text)

        rankings.append({
            'rank': i,
            'title': title.strip() if title else "알 수 없음",
            'url': url,
            'genre': genre
        })

    await page.close()
    return rankings

def extract_genre_from_text(text: str) -> str:
    """텍스트에서 장르 키워드 추출"""
    genres = ['ファンタジー', '恋愛', 'アクション', 'ドラマ', 'ホラー']
    for genre in genres:
        if genre in text:
            return genre
    return ""
```

```python
# db.py
import sqlite3
import json
from pathlib import Path
from utils import get_korean_title, is_riverse_title, translate_genre

DB_PATH = Path(__file__).parent.parent / 'data' / 'rankings.db'

def init_db():
    """DB 초기화 (최초 1회)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS rankings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            date TEXT NOT NULL,
            platform TEXT NOT NULL,
            rank INTEGER NOT NULL,
            title TEXT NOT NULL,
            title_kr TEXT,
            genre TEXT,
            genre_kr TEXT,
            url TEXT,
            thumbnail_url TEXT,
            is_riverse BOOLEAN DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(date, platform, rank)
        )
    ''')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date_platform ON rankings(date, platform)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_riverse ON rankings(is_riverse) WHERE is_riverse = 1')
    conn.commit()
    conn.close()

def save_rankings(date: str, platform: str, rankings: list):
    """랭킹 DB 저장 (upsert)"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    for item in rankings:
        title_kr = get_korean_title(item['title'])
        genre_kr = translate_genre(item.get('genre', ''))
        is_riverse = is_riverse_title(item['title'])

        cursor.execute('''
            INSERT OR REPLACE INTO rankings
            (date, platform, rank, title, title_kr, genre, genre_kr, url, thumbnail_url, is_riverse)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            date, platform, item['rank'], item['title'], title_kr,
            item.get('genre', ''), genre_kr,
            item.get('url', ''), item.get('thumbnail', ''),
            1 if is_riverse else 0
        ))

    conn.commit()
    conn.close()

def backup_to_json(date: str, platform: str, rankings: list):
    """JSON 백업 (SQLite 장애 대비)"""
    backup_dir = Path(__file__).parent.parent / 'data' / 'backup' / date
    backup_dir.mkdir(parents=True, exist_ok=True)

    with open(backup_dir / f'{platform}.json', 'w', encoding='utf-8') as f:
        json.dump(rankings, f, ensure_ascii=False, indent=2)
```

```python
# utils.py
import json
from pathlib import Path

# 참조 데이터 로드
DATA_DIR = Path(__file__).parent.parent / 'data'
with open(DATA_DIR / 'riverse_titles.json', 'r', encoding='utf-8') as f:
    RIVERSE_TITLES = json.load(f)

with open(DATA_DIR / 'title_mappings.json', 'r', encoding='utf-8') as f:
    TITLE_MAPPINGS = json.load(f)

GENRE_TRANSLATIONS = {
    'ファンタジー': '판타지', '恋愛': '연애', 'アクション': '액션',
    'ドラマ': '드라마', 'ホラー': '호러', 'ミステリー': '미스터리',
    # ... (50개 장르 매핑)
}

def get_korean_title(jp_title: str) -> str:
    """일본어 제목 → 한국어 제목 매핑"""
    # 1순위: 리버스 작품
    if jp_title in RIVERSE_TITLES:
        return RIVERSE_TITLES[jp_title]
    # 2순위: 일반 매핑
    if jp_title in TITLE_MAPPINGS:
        return TITLE_MAPPINGS[jp_title]
    # 3순위: 부분 매칭 (4글자 이상)
    for jp, kr in TITLE_MAPPINGS.items():
        if len(jp) >= 4 and jp in jp_title:
            return kr
    return ""

def is_riverse_title(jp_title: str) -> bool:
    """리버스 작품 여부 판별"""
    return jp_title in RIVERSE_TITLES

def translate_genre(jp_genre: str) -> str:
    """일본어 장르 → 한국어"""
    # 복합 장르 처리 (예: "ファンタジー / アクション")
    if ' / ' in jp_genre:
        genres = jp_genre.split(' / ')
        kr_genres = [GENRE_TRANSLATIONS.get(g.strip(), g.strip()) for g in genres]
        return ' / '.join(kr_genres)
    return GENRE_TRANSLATIONS.get(jp_genre.strip(), jp_genre.strip())
```

### 5.3 자동 실행 (macOS cron)

```bash
# crontab -e
# 매일 JST 09:00에 크롤링
0 9 * * * cd ~/webtoon-ranking && /usr/local/bin/python3 crawler/main.py >> logs/cron.log 2>&1
```

---

## 6. 대시보드 (Streamlit)

### 6.1 화면 구성

```python
# dashboard/app.py
import streamlit as st
import sqlite3
import pandas as pd
import plotly.express as px
from datetime import datetime, timedelta

st.set_page_config(
    page_title="일본 웹툰 랭킹 대시보드",
    page_icon="📊",
    layout="wide"
)

# 타이틀
st.title("📊 일본 웹툰 플랫폼 랭킹 대시보드")
st.caption("RIVERSE Inc. 내부 분석 도구 | 매일 09:00 JST 자동 업데이트")

# 날짜 선택
dates = get_available_dates()
selected_date = st.selectbox("📅 날짜 선택", dates, index=0)

# 플랫폼 탭
tab1, tab2, tab3, tab4 = st.tabs([
    "📱 픽코마 (SMARTOON)",
    "💚 라인망가",
    "🔵 메챠코믹",
    "🟠 코믹시모아"
])

platforms = ['piccoma', 'linemanga', 'mechacomic', 'cmoa']
tab_list = [tab1, tab2, tab3, tab4]

for tab, platform in zip(tab_list, platforms):
    with tab:
        # 데이터 로드
        df = load_rankings(selected_date, platform)

        # KPI
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("📚 전체 작품", len(df))
        col2.metric("⭐ 리버스 작품", df['is_riverse'].sum())
        col3.metric("🆕 신규 진입", count_new_entries(df, selected_date))
        col4.metric("📈 평균 순위", f"{df['rank'].mean():.1f}위" if len(df) > 0 else "N/A")

        # 필터
        col_filter1, col_filter2 = st.columns([1, 3])
        with col_filter1:
            show_riverse_only = st.checkbox("리버스 작품만 보기", key=f"filter_{platform}")

        if show_riverse_only:
            df = df[df['is_riverse'] == 1]

        # 랭킹 테이블
        st.dataframe(
            df[[
                'rank', 'rank_change', 'title', 'title_kr',
                'genre_kr', 'url', 'is_riverse'
            ]].rename(columns={
                'rank': '순위',
                'rank_change': '변동',
                'title': '일본어 제목',
                'title_kr': '한국어 제목',
                'genre_kr': '장르',
                'url': '링크',
                'is_riverse': '리버스'
            }),
            use_container_width=True,
            hide_index=True,
            column_config={
                '리버스': st.column_config.CheckboxColumn('⭐'),
                '변동': st.column_config.NumberColumn(
                    format="%d",
                    help="전일 대비 순위 변동 (양수=상승)"
                ),
                '링크': st.column_config.LinkColumn(
                    '🔗',
                    help="작품 페이지로 이동",
                    display_text="보기"
                )
            }
        )

        # 순위 변동 그래프 (핵심 기능!)
        st.divider()
        st.subheader("📈 작품별 순위 변동 추이 (일일 누적)")
        st.caption("매일 크롤링한 데이터를 계속 쌓아서, 특정 작품의 순위 변화를 시간에 따라 그래프로 확인할 수 있습니다.")

        titles = df['title'].tolist()
        selected_title = st.selectbox(
            "작품 선택",
            titles,
            key=f"select_{platform}"
        )

        if selected_title:
            # 최근 30일 히스토리
            history = get_rank_history(selected_title, platform, days=30)
            if not history.empty:
                fig = px.line(
                    history,
                    x='date',
                    y='rank',
                    title=f"{selected_title} 최근 30일 순위 변동",
                    labels={'date': '날짜', 'rank': '순위'},
                    markers=True
                )
                fig.update_yaxis(autorange="reversed")  # 1위가 위로
                fig.update_layout(
                    hovermode='x unified',
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)

                # 통계
                col_stat1, col_stat2, col_stat3 = st.columns(3)
                col_stat1.metric("최고 순위", f"{history['rank'].min()}위")
                col_stat2.metric("최저 순위", f"{history['rank'].max()}위")
                col_stat3.metric("데이터 수", f"{len(history)}일")
            else:
                st.info("📊 히스토리 데이터가 부족합니다. 매일 크롤링이 실행되면 자동으로 그래프가 생성됩니다.")

# 헬퍼 함수들
def get_available_dates():
    """사용 가능한 날짜 목록 (최신순)"""
    conn = sqlite3.connect('../data/rankings.db')
    cursor = conn.cursor()
    cursor.execute('SELECT DISTINCT date FROM rankings ORDER BY date DESC')
    dates = [row[0] for row in cursor.fetchall()]
    conn.close()
    return dates

def load_rankings(date, platform):
    """특정 날짜+플랫폼 랭킹 로드"""
    conn = sqlite3.connect('../data/rankings.db')
    df = pd.read_sql_query(f'''
        SELECT
            rank, title, title_kr, genre, genre_kr,
            url, is_riverse
        FROM rankings
        WHERE date = '{date}' AND platform = '{platform}'
        ORDER BY rank
    ''', conn)
    conn.close()

    # 순위 변동 계산
    df['rank_change'] = calculate_rank_changes(date, platform, df)
    return df

def calculate_rank_changes(date, platform, current_df):
    """전일 대비 순위 변동"""
    conn = sqlite3.connect('../data/rankings.db')
    prev_date = get_previous_date(date)

    if prev_date:
        prev_df = pd.read_sql_query(f'''
            SELECT title, rank FROM rankings
            WHERE date = '{prev_date}' AND platform = '{platform}'
        ''', conn)
        conn.close()

        changes = []
        for _, row in current_df.iterrows():
            prev_rank = prev_df[prev_df['title'] == row['title']]['rank']
            if not prev_rank.empty:
                change = prev_rank.iloc[0] - row['rank']  # 양수 = 상승
                changes.append(change)
            else:
                changes.append(999)  # NEW
        return changes
    else:
        conn.close()
        return [0] * len(current_df)

def get_rank_history(title, platform, days=30):
    """작품별 순위 히스토리"""
    conn = sqlite3.connect('../data/rankings.db')
    df = pd.read_sql_query(f'''
        SELECT date, rank
        FROM rankings
        WHERE title = '{title}' AND platform = '{platform}'
        ORDER BY date DESC
        LIMIT {days}
    ''', conn)
    conn.close()
    return df.sort_values('date')

def count_new_entries(df, date):
    """신규 진입 작품 수"""
    prev_date = get_previous_date(date)
    if not prev_date:
        return 0

    conn = sqlite3.connect('../data/rankings.db')
    prev_titles = pd.read_sql_query(f'''
        SELECT DISTINCT title FROM rankings WHERE date = '{prev_date}'
    ''', conn)
    conn.close()

    new_count = 0
    for title in df['title']:
        if title not in prev_titles['title'].values:
            new_count += 1
    return new_count
```

### 6.2 실행 방법

```bash
# 로컬 실행
cd dashboard
streamlit run app.py

# → 브라우저 자동 오픈: http://localhost:8501
```

### 6.3 무료 배포 (Streamlit Cloud)

```bash
# 1. GitHub에 푸시
git add .
git commit -m "Add dashboard"
git push origin main

# 2. Streamlit Cloud (https://share.streamlit.io) 접속
# 3. "New app" → 저장소 선택 → dashboard/app.py 선택
# 4. 자동 배포 → 무료 URL 발급: https://webtoon-ranking.streamlit.app
```

---

## 7. 배포 및 운영 (맥북)

### 7.1 초기 설정 (30분, 최초 1회)

```bash
# 1. 프로젝트 클론
cd ~/Documents
git clone [GitHub URL] webtoon-ranking
cd webtoon-ranking

# 2. Python 가상환경
python3 -m venv venv
source venv/bin/activate

# 3. 의존성 설치
pip install playwright beautifulsoup4 requests
playwright install chromium

# 4. DB 초기화
python3 -c "from crawler.db import init_db; init_db()"

# 5. 테스트 크롤링
python3 crawler/main.py

# 6. cron 등록
crontab -e
# 추가: 0 9 * * * cd ~/Documents/webtoon-ranking && ~/Documents/webtoon-ranking/venv/bin/python3 crawler/main.py >> logs/cron.log 2>&1

# 7. 로그 디렉토리
mkdir -p logs
```

### 7.2 일상 운영

**수동 크롤링**:
```bash
cd ~/Documents/webtoon-ranking
source venv/bin/activate
python3 crawler/main.py
```

**대시보드 실행**:
```bash
cd ~/Documents/webtoon-ranking/dashboard
streamlit run app.py
```

**로그 확인**:
```bash
tail -f ~/Documents/webtoon-ranking/logs/cron.log
```

**DB 백업**:
```bash
cp data/rankings.db ~/Backups/rankings_$(date +%Y%m%d).db
```

### 7.3 문제 해결

| 증상 | 원인 | 해결 |
|------|------|------|
| Playwright 에러 | Chromium 미설치 | `playwright install chromium` |
| IP 403 에러 | 일본 IP 아님 | 맥북을 일본 네트워크에 연결 |
| cron 미작동 | 권한 문제 | 시스템 설정 → 개인정보보호 → 전체 디스크 접근 → Terminal 추가 |
| DB 락 에러 | 동시 쓰기 | cron 시간 조정 (중복 실행 방지) |

---

## 8. 데이터 활용 예시

### 8.1 질문별 활용법

**Q1. 우리 작품 "나 혼자만 레벨업"이 지금 픽코마에서 몇 위?**
→ 대시보드 → 픽코마 탭 → "리버스 작품만 보기" 체크 → 확인

**Q2. 지난 1개월간 순위 변동 추이는?**
→ 대시보드 → 작품 선택 → 그래프 확인

**Q3. 경쟁작 "転生したらスライムだった件"이 어떤 플랫폼에서 강세?**
→ 4개 탭 돌아다니며 검색 또는 DB 직접 쿼리

**Q4. 이번 주 신규 진입 작품은?**
→ 대시보드 상단 KPI의 "신규 진입" 숫자 확인

**Q5. 월간 리포트 만들어야 하는데 데이터 다운로드?**
→ SQLite DB 파일 복사 또는 JSON 백업 폴더 압축

---

## 9. 비용 및 리소스

| 항목 | 비용 | 비고 |
|------|------|------|
| **크롤링 서버** | $0 | 이미 있는 맥북 활용 |
| **데이터베이스** | $0 | SQLite (로컬 파일) |
| **대시보드 호스팅** | $0 | Streamlit Cloud 무료 티어 |
| **개발 비용** | $0 | 내부 개발 |
| **유지보수** | 월 1시간 | 로그 확인, 에러 수정 |
| **총 운영 비용** | **$0/년** | 100% 무료 |

**맥북 리소스**:
- CPU: 크롤링 2-3분간 50-80% 사용
- 메모리: 2-3GB (Playwright 브라우저)
- 디스크: 1년치 데이터 ~50MB

---

## 10. 개발 일정 (3일)

### Day 1: 크롤러 구현
- [ ] 프로젝트 구조 생성
- [ ] SQLite 스키마 작성
- [ ] 픽코마 크롤러 (SSR, 가장 쉬움)
- [ ] 라인망가 크롤러 (CSR, 무한 스크롤)
- [ ] DB 저장 로직 + JSON 백업

### Day 2: 크롤러 완성 + 대시보드 시작
- [ ] 메챠코믹 크롤러 (페이지네이션)
- [ ] 코믹시모아 크롤러 (TLS 이슈)
- [ ] 한국어 제목 매핑 로직
- [ ] 리버스 작품 판별 로직
- [ ] Streamlit 기본 UI

### Day 3: 대시보드 완성 + 배포
- [ ] 4개 플랫폼 탭 구현
- [ ] 순위 변동 그래프 (Plotly)
- [ ] 리버스 작품 하이라이트
- [ ] cron job 설정
- [ ] 테스트 및 버그 수정
- [ ] 문서화 (README.md)

---

## 11. 향후 확장 (우선순위 낮음)

### 단기 (필요 시)
- [ ] Slack 알림 (크롤링 완료/실패)
- [ ] Excel 다운로드 기능
- [ ] 장르별 필터링

### 중장기 (6개월+)
- [ ] 장르별 랭킹 추가 수집
- [ ] 신작 랭킹 추가
- [ ] 주간/월간 자동 리포트
- [ ] 경쟁작 비교 분석
- [ ] 라인망가 앱 버전 크롤링

---

## 12. 프로젝트 파일 구조

```
webtoon-ranking/
├── crawler/
│   ├── main.py              # 크롤러 메인
│   ├── platforms/
│   │   ├── __init__.py
│   │   ├── piccoma.py
│   │   ├── linemanga.py
│   │   ├── mechacomic.py
│   │   └── cmoa.py
│   ├── db.py                # SQLite 헬퍼
│   └── utils.py             # 공통 유틸
│
├── dashboard/
│   └── app.py               # Streamlit 대시보드
│
├── data/
│   ├── rankings.db          # SQLite DB
│   ├── riverse_titles.json  # 리버스 407개
│   ├── title_mappings.json  # 한국어 3,135개
│   └── backup/              # JSON 백업
│       └── 2026-02-15/
│           ├── piccoma.json
│           ├── linemanga.json
│           ├── mechacomic.json
│           └── cmoa.json
│
├── logs/
│   └── cron.log
│
├── requirements.txt
├── README.md
└── .gitignore
```

---

## 13. 마무리

### 13.1 핵심 요약
- ✅ **간단함**: Python 900줄, 누구나 이해 가능
- ✅ **무료**: 클라우드 비용 $0
- ✅ **빠름**: 3일 개발, 매일 2-3분 크롤링
- ✅ **안정적**: SQLite + JSON 이중 백업
- ✅ **실용적**: 일본 팀 + 한국 팀 모두 사용 가능

### 13.2 vs 마누스 기획서

| 항목 | 마누스 v5.0 | 실용 v2.0 (본 기획서) |
|------|------------|---------------------|
| 기술 스택 | TypeScript + React + tRPC + Drizzle | Python + Streamlit |
| 코드량 | 6,100줄 | 900줄 |
| 파일 수 | 20+ | 8개 |
| DB | TiDB (클라우드, $25/월) | SQLite (무료) |
| 배포 복잡도 | launchd + cron + 10단계 | cron 1줄 |
| 개발 기간 | 2주 | 3일 |
| 비용 | $660/년 | $0/년 |

**결론**: 85% 단순하고, 100% 무료이며, 3배 빠른 개발

---

*본 기획서는 실용성과 유지보수성을 최우선으로 설계되었습니다.
마누스 AI의 과도하게 복잡한 설계를 버리고, 실제 필요에 맞춘 최적화된 솔루션입니다.*

**문의**: RIVERSE Inc. 기술팀
