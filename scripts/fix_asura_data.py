#!/usr/bin/env python3
"""
Asura ë°ì´í„° ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸:
1. ì¤‘ë³µ ì œëª© ë³‘í•© (truncated vs full)
2. ë¯¸ë§¤í•‘ Asura ì˜ë¬¸ ì œëª© â†’ title_kr ìƒì„± (fallback: ì˜ë¬¸ ê·¸ëŒ€ë¡œ)
3. unified_work_id ë°±í•„
4. title_en ë°±í•„
"""
import json
import os
import re
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler.db import get_db_connection, _upsert_unified_work
from crawler.utils import get_korean_title, is_riverse_title


def normalize_title(title: str) -> str:
    """ì œëª© ì •ê·œí™” (ë¹„êµìš©)"""
    return re.sub(r'[^a-z0-9]', '', title.lower())


def step1_merge_duplicates():
    """Step 1: Asura ì¤‘ë³µ ì œëª© ë³‘í•©"""
    print("\n" + "=" * 60)
    print("Step 1: Asura ì¤‘ë³µ ì œëª© ë³‘í•©")
    print("=" * 60)

    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("SELECT id, title, url, author, description, rating FROM works WHERE platform = 'asura'")
    works = cur.fetchall()

    # ì •ê·œí™” ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘
    groups = {}
    for wid, title, url, author, desc, rating in works:
        norm = normalize_title(title)
        if norm not in groups:
            groups[norm] = []
        groups[norm].append({
            'id': wid, 'title': title, 'url': url,
            'author': author or '', 'desc': desc or '', 'rating': rating
        })

    merged = 0
    for norm, group in groups.items():
        if len(group) <= 1:
            continue

        # ê°€ì¥ ì¢‹ì€ í•­ëª© ì„ íƒ: ê¸´ ì œëª© + ë°ì´í„°ê°€ ë§ì€ ê²ƒ
        group.sort(key=lambda x: (
            len(x['author']) + len(x['desc']),  # ë°ì´í„° í’ë¶€í•¨
            len(x['title']),                      # ê¸´ ì œëª©
            -x['id']                               # ìµœì‹  ID
        ), reverse=True)
        keep = group[0]
        removes = group[1:]

        for rm in removes:
            print(f"  ë³‘í•©: \"{rm['title'][:50]}\" â†’ \"{keep['title'][:50]}\"")

            # 1. reviews ì´ë™
            cur.execute("""
                UPDATE reviews SET work_title = %s
                WHERE platform = 'asura' AND work_title = %s
                AND NOT EXISTS (
                    SELECT 1 FROM reviews r2
                    WHERE r2.platform = 'asura' AND r2.work_title = %s
                      AND r2.reviewer_name = reviews.reviewer_name
                      AND r2.reviewed_at = reviews.reviewed_at
                )
            """, (keep['title'], rm['title'], keep['title']))

            # 2. rankings ì´ë™
            cur.execute("""
                UPDATE rankings SET title = %s
                WHERE platform = 'asura' AND title = %s
                AND NOT EXISTS (
                    SELECT 1 FROM rankings r2
                    WHERE r2.platform = 'asura' AND r2.title = %s
                      AND r2.date = rankings.date
                      AND r2.sub_category = rankings.sub_category
                      AND r2.rank = rankings.rank
                )
            """, (keep['title'], rm['title'], keep['title']))

            # 3. ë‚¨ì€ orphan ì‚­ì œ
            cur.execute("DELETE FROM reviews WHERE platform = 'asura' AND work_title = %s", (rm['title'],))
            cur.execute("DELETE FROM rankings WHERE platform = 'asura' AND title = %s", (rm['title'],))

            # 4. works í–‰ ì‚­ì œ
            cur.execute("DELETE FROM works WHERE id = %s", (rm['id'],))
            merged += 1

    conn.commit()
    conn.close()
    print(f"âœ… ì¤‘ë³µ ë³‘í•© ì™„ë£Œ: {merged}ê°œ ì œê±°")


def step2_generate_mappings():
    """Step 2: ë¯¸ë§¤í•‘ Asura ì‘í’ˆì— ëŒ€í•´ title_kr ë§¤í•‘ ìƒì„±"""
    print("\n" + "=" * 60)
    print("Step 2: Asura ì˜ë¬¸ ì œëª© ë§¤í•‘ ìƒì„±")
    print("=" * 60)

    conn = get_db_connection()
    cur = conn.cursor()

    # ë§¤í•‘ ì—†ëŠ” Asura works
    cur.execute("""
        SELECT title FROM works
        WHERE platform = 'asura' AND (title_kr IS NULL OR title_kr = '')
        ORDER BY title
    """)
    unlinked = [r[0] for r in cur.fetchall()]
    print(f"ë§¤í•‘ í•„ìš”: {len(unlinked)}ê°œ")

    if not unlinked:
        conn.close()
        return

    # unified_worksì—ì„œ í¬ë¡œìŠ¤ ë ˆí¼ëŸ°ìŠ¤ ê²€ìƒ‰
    cur.execute("SELECT id, title_kr, title_canonical, title_en FROM unified_works")
    unified = cur.fetchall()

    # title_mappings.json ë¡œë“œ
    mappings_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                  'data', 'title_mappings.json')
    with open(mappings_path, 'r', encoding='utf-8') as f:
        mappings = json.load(f)

    new_mappings = {}
    fallback_count = 0
    cross_ref_count = 0

    for en_title in unlinked:
        en_norm = normalize_title(en_title)

        # ë°©ë²• 1: unified_works.title_canonical ë˜ëŠ” title_enê³¼ ë§¤ì¹­
        found = False
        for uid, tkr, tcanon, ten in unified:
            if tcanon and normalize_title(tcanon) == en_norm:
                new_mappings[en_title] = tkr
                cross_ref_count += 1
                found = True
                break
            if ten and normalize_title(ten) == en_norm:
                new_mappings[en_title] = tkr
                cross_ref_count += 1
                found = True
                break

        if found:
            continue

        # ë°©ë²• 2: ê¸°ì¡´ ë§¤í•‘ì˜ ê°’ì—ì„œ ì—­ë°©í–¥ ê²€ìƒ‰ (EN value â†’ KR)
        for key, kr in mappings.items():
            if normalize_title(key) == en_norm:
                new_mappings[en_title] = kr
                cross_ref_count += 1
                found = True
                break

        if found:
            continue

        # ë°©ë²• 3: ì˜ë¬¸ ì œëª© ê·¸ëŒ€ë¡œ title_krë¡œ ì‚¬ìš© (fallback)
        new_mappings[en_title] = en_title
        fallback_count += 1

    # title_mappings.json ì—…ë°ì´íŠ¸
    mappings.update(new_mappings)
    with open(mappings_path, 'w', encoding='utf-8') as f:
        json.dump(mappings, f, ensure_ascii=False, indent=2, sort_keys=True)

    conn.close()
    print(f"âœ… ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(new_mappings)}ê°œ")
    print(f"   í¬ë¡œìŠ¤ ë ˆí¼ëŸ°ìŠ¤: {cross_ref_count}ê°œ")
    print(f"   ì˜ë¬¸ fallback: {fallback_count}ê°œ")


def step3_backfill_links():
    """Step 3: ëª¨ë“  Asura worksì— unified_work_id + title_kr ë°±í•„"""
    print("\n" + "=" * 60)
    print("Step 3: Asura unified_work_id ë°±í•„")
    print("=" * 60)

    # utils ëª¨ë“ˆì˜ ìºì‹œ ì´ˆê¸°í™” (ìƒˆ ë§¤í•‘ ë°˜ì˜)
    from crawler import utils
    utils._title_mappings = None

    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("""
        SELECT id, title, title_kr, unified_work_id
        FROM works WHERE platform = 'asura'
        ORDER BY title
    """)
    works = cur.fetchall()

    linked = 0
    updated = 0
    for wid, title, existing_kr, existing_uwid in works:
        title_kr = get_korean_title(title)

        if not title_kr:
            # ë§¤í•‘ì—ë„ ì—†ìœ¼ë©´ ì˜ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            title_kr = title

        is_riv = is_riverse_title(title)

        # unified_work upsert
        uwid = _upsert_unified_work(
            cur, title_kr, title,
            is_riverse=is_riv,
            title_en=title,
        )

        if uwid:
            # works í–‰ ì—…ë°ì´íŠ¸
            cur.execute("""
                UPDATE works SET
                    unified_work_id = %s,
                    title_kr = %s,
                    is_riverse = is_riverse OR %s,
                    updated_at = NOW()
                WHERE id = %s
            """, (uwid, title_kr, is_riv, wid))

            if not existing_uwid:
                linked += 1
            else:
                updated += 1

    conn.commit()
    conn.close()
    print(f"âœ… ë°±í•„ ì™„ë£Œ: ì‹ ê·œ ë§í¬ {linked}ê°œ, ì—…ë°ì´íŠ¸ {updated}ê°œ")


def step4_backfill_title_en():
    """Step 4: ê¸°ì¡´ unified_worksì— Asura ì˜ë¬¸ ì œëª© ë°±í•„"""
    print("\n" + "=" * 60)
    print("Step 4: title_en ë°±í•„")
    print("=" * 60)

    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("""
        UPDATE unified_works uw
        SET title_en = w.title, updated_at = NOW()
        FROM works w
        WHERE w.unified_work_id = uw.id
          AND w.platform = 'asura'
          AND (uw.title_en IS NULL OR uw.title_en = '')
    """)
    count = cur.rowcount
    conn.commit()
    conn.close()
    print(f"âœ… title_en ë°±í•„ ì™„ë£Œ: {count}ê°œ ì—…ë°ì´íŠ¸")


def verify():
    """ê²€ì¦"""
    print("\n" + "=" * 60)
    print("ê²€ì¦")
    print("=" * 60)

    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("""
        SELECT COUNT(*), COUNT(unified_work_id),
               COUNT(CASE WHEN title_kr <> '' AND title_kr IS NOT NULL THEN 1 END)
        FROM works WHERE platform = 'asura'
    """)
    r = cur.fetchone()
    print(f"Asura works: {r[0]}ê°œ total, linked={r[1]}, title_kr={r[2]}")

    cur.execute("SELECT COUNT(*) FROM unified_works WHERE title_en <> '' AND title_en IS NOT NULL")
    print(f"unified_works with title_en: {cur.fetchone()[0]}ê°œ")

    # ë‚˜ë…¸ë§ˆì‹  ì²´í¬
    cur.execute("""
        SELECT w.platform, w.title, w.unified_work_id, uw.title_kr, uw.title_en
        FROM works w
        LEFT JOIN unified_works uw ON w.unified_work_id = uw.id
        WHERE (w.title ILIKE '%nano machine%' OR uw.title_kr LIKE '%ë‚˜ë…¸ë§ˆì‹ %')
        ORDER BY w.platform
    """)
    print(f"\në‚˜ë…¸ë§ˆì‹  í¬ë¡œìŠ¤ í”Œë«í¼:")
    for r in cur.fetchall():
        print(f"  {r[0]:12} {r[1][:30]:30} uwid={r[2]} kr={r[3][:15] if r[3] else ''} en={r[4][:20] if r[4] else ''}")

    conn.close()


if __name__ == '__main__':
    print("ğŸ”§ Asura ë°ì´í„° ìˆ˜ì • ì‹œì‘")
    step1_merge_duplicates()
    step2_generate_mappings()
    step3_backfill_links()
    step4_backfill_title_en()
    verify()
    print("\nâœ… ëª¨ë“  ìˆ˜ì • ì™„ë£Œ!")
