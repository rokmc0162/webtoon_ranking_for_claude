"""
ì½”ë¯¹ì‹œëª¨ì•„ ë¼ì´íŠ¸ ì–´ëœíŠ¸ (ãƒ©ã‚¤ãƒˆã‚¢ãƒ€ãƒ«ãƒˆ) ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- ê¸°ì¡´ cmoa_agentì™€ ë™ì¼í•œ DOM êµ¬ì¡°
- /search/purpose/ranking/sexy/ ê²½ë¡œ ì‚¬ìš©
- IP ì œí•œ ì—†ìŒ
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class CmoaSexyAgent(CrawlerAgent):
    """ì½”ë¯¹ì‹œëª¨ì•„ ë¼ì´íŠ¸ ì–´ëœíŠ¸(sexy) ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    def __init__(self):
        super().__init__(
            platform_id='cmoa_sexy',
            platform_name='ì½”ë¯¹ì‹œëª¨ì•„ (ë¼ì´íŠ¸ì–´ëœíŠ¸)',
            url='https://www.cmoa.jp/search/purpose/ranking/sexy/'
        )

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ì½”ë¯¹ì‹œëª¨ì•„ ë¼ì´íŠ¸ ì–´ëœíŠ¸ ë­í‚¹ í¬ë¡¤ë§"""
        context = await browser.new_context(ignore_https_errors=True)
        page = await context.new_page()

        try:
            self.logger.info(f"ğŸ“± ì½”ë¯¹ì‹œëª¨ì•„ [ë¼ì´íŠ¸ ì–´ëœíŠ¸] í¬ë¡¤ë§ ì¤‘... â†’ {self.url}")

            await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
            await page.wait_for_selector('li.search_result_box', timeout=10000)
            await page.wait_for_timeout(1000)

            items = await page.query_selector_all('li.search_result_box')
            self.logger.info(f"   ì‘í’ˆ ìš”ì†Œ {len(items)}ê°œ ë°œê²¬")

            rankings = []
            for item in items[:100]:
                try:
                    entry = await self._parse_item(item)
                    if entry:
                        rankings.append(entry)
                except Exception as e:
                    self.logger.debug(f"ì‘í’ˆ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            rankings.sort(key=lambda x: x['rank'])
            self.logger.info(f"   âœ… [ë¼ì´íŠ¸ ì–´ëœíŠ¸]: {len(rankings)}ê°œ ì‘í’ˆ")

            return rankings

        finally:
            await page.close()
            await context.close()

    async def _parse_item(self, item) -> Dict[str, Any]:
        """ê°œë³„ ë­í‚¹ ì•„ì´í…œ íŒŒì‹± (cmoa_agentì™€ ë™ì¼ êµ¬ì¡°)"""

        # 1. ìˆœìœ„
        rank_el = await item.query_selector('.title_rank')
        if not rank_el:
            return None
        rank_text = (await rank_el.inner_text()).strip()
        match = re.search(r'(\d+)ä½', rank_text)
        if not match:
            return None
        rank = int(match.group(1))

        # 2. ì œëª©
        title = None
        title_el = await item.query_selector('.search_result_box_right_sec1 a.title')
        if title_el:
            title = (await title_el.inner_text()).strip()
        if not title:
            img_el = await item.query_selector('img.volume_img')
            if img_el:
                title = await img_el.get_attribute('alt')
        if not title:
            return None

        # 3. URL
        url = ''
        if title_el:
            href = await title_el.get_attribute('href')
            if href:
                url = f"https://www.cmoa.jp{href}" if not href.startswith('http') else href

        # 4. ì¥ë¥´
        genre = ''
        sec2 = await item.query_selector('.search_result_box_right_sec2')
        if sec2:
            sec2_html = await sec2.inner_html()
            genre_match = re.search(r'ã‚¸ãƒ£ãƒ³ãƒ«ï¼š\s*<a[^>]*>([^<]+)</a>', sec2_html)
            if genre_match:
                genre = genre_match.group(1).strip()

        # 5. ì¸ë„¤ì¼
        thumbnail_url = ''
        thumb_el = await item.query_selector('img.volume_img')
        if thumb_el:
            thumb_src = await thumb_el.get_attribute('data-src') or ''
            if not thumb_src:
                thumb_src = await thumb_el.get_attribute('src') or ''
            if thumb_src and 'loader.png' not in thumb_src:
                thumbnail_url = f"https:{thumb_src}" if thumb_src.startswith('//') else thumb_src

        return {
            'rank': rank,
            'title': title.strip(),
            'genre': genre,
            'url': url,
            'thumbnail_url': thumbnail_url,
        }

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ë­í‚¹ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)
