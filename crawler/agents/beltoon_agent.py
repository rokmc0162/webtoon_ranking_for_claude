"""
ë²¨íˆ° (BeLTOON) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- CSR ë°©ì‹ (Next.js + styled-components)
- IP ì œí•œ ì—†ìŒ
- í•´ì‹œëœ í´ë˜ìŠ¤ëª… â†’ êµ¬ì¡° ê¸°ë°˜ ì…€ë ‰í„° ì‚¬ìš©
- li > div > span > div > img íŒ¨í„´, image.balcony.studio ë„ë©”ì¸
- ì¢…í•©: /app/all/ranking URL ì§ì ‘ ì ‘ê·¼
- ì¥ë¥´ë³„: í•„í„°(çµã‚Šè¾¼ã¿) UIì—ì„œ ì¥ë¥´ íƒœê·¸ ì„ íƒ í›„ ì ìš©
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class BeltoonAgent(CrawlerAgent):
    """ë²¨íˆ° ë°ì¼ë¦¬ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©(ë°ì¼ë¦¬)', 'slug': 'all'},
        'ãƒ­ãƒãƒ³ã‚¹': {'name': 'ë¡œë§ŒìŠ¤', 'filter_tag': 'ãƒ­ãƒãƒ³ã‚¹'},
    }

    def __init__(self):
        super().__init__(
            platform_id='beltoon',
            platform_name='ë²¨íˆ° (BeLTOON)',
            url='https://www.beltoon.jp/app/all/ranking'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ë²¨íˆ° ì¢…í•© + ì¥ë¥´ë³„ ë°ì¼ë¦¬ ë­í‚¹ í¬ë¡¤ë§"""
        page = await browser.new_page()
        all_rankings = []

        try:
            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']

                self.logger.info(f"ğŸ“± ë²¨íˆ° [{label}] í¬ë¡¤ë§ ì¤‘...")

                try:
                    if genre_key == '':
                        # ì¢…í•©: URL ì§ì ‘ ì ‘ê·¼
                        rankings = await self._crawl_all(page)
                    else:
                        # ì¥ë¥´ë³„: í•„í„° UIë¡œ ì¥ë¥´ ì„ íƒ
                        filter_tag = genre_info['filter_tag']
                        rankings = await self._crawl_filtered(page, filter_tag)

                    self.genre_results[genre_key] = rankings
                    self.logger.info(f"   âœ… [{label}]: {len(rankings)}ê°œ ì‘í’ˆ")
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ [{label}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    self.genre_results[genre_key] = []
                    continue

                # ì¢…í•© ë­í‚¹ì€ ë°˜í™˜ê°’ìœ¼ë¡œ ì‚¬ìš©
                if genre_key == '':
                    all_rankings = rankings

            return all_rankings

        finally:
            await page.close()

    async def _crawl_all(self, page) -> List[Dict[str, Any]]:
        """ì¢…í•© ë­í‚¹: URL ì§ì ‘ ì ‘ê·¼"""
        await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
        await page.wait_for_timeout(5000)

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ lazy loading íŠ¸ë¦¬ê±°
        for _ in range(10):
            await page.evaluate('window.scrollBy(0, 1000)')
            await page.wait_for_timeout(500)

        rankings = await self._parse_dom_rankings(page)

        if len(rankings) < 5:
            self.logger.info(f"   DOM íŒŒì‹± ë¶€ì¡±, í…ìŠ¤íŠ¸ í´ë°±...")
            body_text = await page.inner_text('body')
            rankings = self._parse_text_rankings(body_text)

        return rankings

    async def _crawl_filtered(self, page, filter_tag: str) -> List[Dict[str, Any]]:
        """ì¥ë¥´ë³„ ë­í‚¹: í•„í„° UIì—ì„œ ì¥ë¥´ ì²´í¬ë°•ìŠ¤ ì„ íƒ"""
        # ì¢…í•© ë­í‚¹ í˜ì´ì§€ë¡œ ì´ë™ (í•„í„° ì´ˆê¸°í™”)
        await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
        await page.wait_for_timeout(5000)

        # 1. çµã‚Šè¾¼ã¿(í•„í„°) ë²„íŠ¼ í´ë¦­
        filter_btn = await page.query_selector('text="çµã‚Šè¾¼ã¿"')
        if not filter_btn:
            self.logger.warning("   âš ï¸ çµã‚Šè¾¼ã¿ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        await filter_btn.click()
        await page.wait_for_timeout(2000)
        self.logger.info(f"   ğŸ” í•„í„° íŒì—… ì—´ê¸°")

        # 2. ì¥ë¥´ ì²´í¬ë°•ìŠ¤: filter_tagë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ í•´ì œ
        genre_labels = await page.query_selector_all('label[data-type="genre"]')
        for label in genre_labels:
            text_span = await label.query_selector('.text')
            text = await text_span.inner_text() if text_span else ''
            checkbox = await label.query_selector('input[type="checkbox"]')
            is_checked = await checkbox.is_checked() if checkbox else False

            if text == filter_tag:
                # ì›í•˜ëŠ” ì¥ë¥´ëŠ” ì²´í¬ ìœ ì§€
                if not is_checked:
                    await label.click()
                    await page.wait_for_timeout(200)
                self.logger.info(f"   ğŸ·ï¸ {text} ì²´í¬ ìœ ì§€")
                continue

            # ë‚˜ë¨¸ì§€ ì¥ë¥´ í•´ì œ
            if is_checked:
                await label.click()
                await page.wait_for_timeout(200)

        await page.wait_for_timeout(500)

        # 3. çµã‚Šè¾¼ã‚€(ì ìš©) ë²„íŠ¼ í´ë¦­ - ê²€ìƒ‰ê²°ê³¼ ê±´ìˆ˜ ë¡œê·¸
        apply_btn = await page.query_selector('button:has-text("çµã‚Šè¾¼ã‚€")')
        if not apply_btn:
            self.logger.warning("   âš ï¸ çµã‚Šè¾¼ã‚€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
        btn_text = await apply_btn.inner_text()
        self.logger.info(f"   ğŸ” {btn_text}")
        await apply_btn.click()
        await page.wait_for_timeout(5000)

        # 4. ìŠ¤í¬ë¡¤ ë‹¤ìš´
        for _ in range(10):
            await page.evaluate('window.scrollBy(0, 1000)')
            await page.wait_for_timeout(500)

        # 5. DOM íŒŒì‹±
        rankings = await self._parse_dom_rankings(page)

        if len(rankings) < 1:
            self.logger.info(f"   DOM íŒŒì‹± ë¶€ì¡±, í…ìŠ¤íŠ¸ í´ë°±...")
            body_text = await page.inner_text('body')
            rankings = self._parse_text_rankings(body_text)

        return rankings

    async def _parse_dom_rankings(self, page) -> List[Dict[str, Any]]:
        """DOMì—ì„œ ë­í‚¹ ì•„ì´í…œ + ì¸ë„¤ì¼ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            // beltoon: styled-components, êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ
            // li ì•ˆì— img[alt="thumbnail"] + í…ìŠ¤íŠ¸ êµ¬ì¡°
            const allLis = document.querySelectorAll('li');
            let rank = 0;

            for (const li of allLis) {
                const img = li.querySelector('img[alt="thumbnail"], img[src*="balcony.studio"], img[src*="image."]');
                if (!img) continue;

                const src = img.getAttribute('src') || '';
                if (!src || src.includes('logo') || src.includes('icon')) continue;

                // íƒ€ì´í‹€: li ë‚´ì˜ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¤‘ ì ì ˆí•œ ê²ƒ ì°¾ê¸°
                // ë³´í†µ img ì´í›„ì— span ë˜ëŠ” p ë¡œ íƒ€ì´í‹€ì´ ìˆìŒ
                let title = '';
                const textEls = li.querySelectorAll('span, p, h3, h4, div');
                for (const el of textEls) {
                    const text = el.textContent.trim();
                    // ìˆ«ìë§Œ(ìˆœìœ„), ì¡°íšŒìˆ˜(Në§Œ), ì‘ê°€ëª…ì€ ìŠ¤í‚µ
                    if (text.length >= 3 && text.length <= 100 &&
                        !/^\\d+$/.test(text) &&
                        !/^[\\d.]+ë§Œ$/.test(text) &&
                        !text.includes('ãƒã‚§ãƒƒã‚¯') &&
                        !text.includes('ãƒ­ãƒãƒ³ã‚¹') &&
                        !text.includes('BL') &&
                        el.children.length === 0) {
                        title = text;
                        break;
                    }
                }

                if (!title || title.length < 2) continue;

                // URL
                const linkEl = li.querySelector('a');
                const href = linkEl ? linkEl.getAttribute('href') : '';
                const fullUrl = href ? (href.startsWith('http') ? href : 'https://www.beltoon.jp' + href) : '';

                const thumbUrl = src.startsWith('http') ? src : (src.startsWith('/') ? 'https://www.beltoon.jp' + src : '');

                rank++;
                if (rank <= 100) {
                    results.push({
                        rank: rank,
                        title: title,
                        url: fullUrl,
                        thumbnail_url: thumbUrl,
                    });
                }
            }
            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': '',
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    def _parse_text_rankings(self, body_text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ - í´ë°±"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []

        start_idx = 0
        for i, line in enumerate(lines):
            if line == 'ãƒ‡ã‚¤ãƒªãƒ¼':
                start_idx = i + 1
                break

        i = start_idx
        while i < len(lines) and len(rankings) < 100:
            line = lines[i]

            if line.isdigit() and 1 <= int(line) <= 100:
                rank = int(line)
                j = i + 1
                while j < len(lines) and lines[j].isdigit():
                    j += 1

                if j < len(lines):
                    title = lines[j].strip()
                    if (len(title) >= 2 and
                            title not in ['ãƒã‚§ãƒƒã‚¯è§£é™¤', 'çµã‚Šè¾¼ã¿', '...', 'ãƒ­ãƒãƒ³ã‚¹',
                                          'BL', 'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼', 'ãƒ‰ãƒ©ãƒ', 'GL', 'å°‘å¥³ãƒãƒ³ã‚¬'] and
                            not title.startswith('(')):
                        rankings.append({
                            'rank': rank,
                            'title': title,
                            'genre': '',
                            'url': 'https://www.beltoon.jp/app/all/ranking',
                            'thumbnail_url': '',
                        })
                        i = j + 1
                        continue

            i += 1

        return rankings

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        # ì¢…í•© ë­í‚¹ ì €ì¥
        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        # ì¥ë¥´ë³„ ë­í‚¹ ì €ì¥
        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            genre_meta = [
                {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
                 'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
                for item in rankings if item.get('thumbnail_url')
            ]
            if genre_meta:
                save_works_metadata(self.platform_id, genre_meta, date=date, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
