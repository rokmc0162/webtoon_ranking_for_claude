"""
U-NEXT í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- CSR ë°©ì‹ (Next.js + Apollo GraphQL)
- IP ì œí•œ ì—†ìŒ
- styled-components â†’ picture > img êµ¬ì¡°, metac.nxtv.jp ë„ë©”ì¸
- ë§Œí™” ë­í‚¹ (/book/ranking/comic)
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class UnextAgent(CrawlerAgent):
    """U-NEXT ë§Œí™” ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    def __init__(self):
        super().__init__(
            platform_id='unext',
            platform_name='U-NEXT (ë§Œí™”)',
            url='https://video.unext.jp/book/ranking/comic'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """U-NEXT ë§Œí™” ë­í‚¹ í¬ë¡¤ë§ - GraphQL route interceptë¡œ 5í˜ì´ì§€ ìˆ˜ì§‘"""
        import json
        import urllib.parse

        context = browser.contexts[0] if browser.contexts else await browser.new_context()
        page = await context.new_page()

        try:
            self.logger.info(f"ğŸ“± U-NEXT [ë§Œí™”] í¬ë¡¤ë§ ì¤‘... â†’ {self.url}")

            all_books = []
            current_page_target = [1]  # mutable for closure

            # GraphQL ì‘ë‹µ ìº¡ì²˜
            async def on_response(response):
                if 'cosmo_getBookRanking' in response.url:
                    try:
                        body = await response.json()
                        books = body.get('data', {}).get('bookRanking', {}).get('books', [])
                        for book in books:
                            s = book.get('bookSakuhin', {})
                            if not s or not s.get('name'):
                                continue
                            # ì¸ë„¤ì¼: book.thumbnail.standard
                            thumb = s.get('book', {}).get('thumbnail', {}).get('standard', '')
                            if thumb and not thumb.startswith('http'):
                                thumb = 'https://' + thumb
                            all_books.append({
                                'name': s.get('name', ''),
                                'code': s.get('sakuhinCode', ''),
                                'thumb': thumb,
                            })
                    except Exception:
                        pass

            page.on('response', on_response)

            # Route intercept: page íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ íƒ€ê²Ÿìœ¼ë¡œ ë³€ê²½
            async def modify_request(route):
                url = route.request.url
                if 'cosmo_getBookRanking' in url and current_page_target[0] > 1:
                    parsed = urllib.parse.urlparse(url)
                    params = urllib.parse.parse_qs(parsed.query)
                    if 'variables' in params:
                        variables = json.loads(params['variables'][0])
                        variables['page'] = current_page_target[0]
                        params['variables'] = [json.dumps(variables)]
                        new_query = urllib.parse.urlencode({k: v[0] for k, v in params.items()})
                        new_url = f'{parsed.scheme}://{parsed.netloc}{parsed.path}?{new_query}'
                        await route.continue_(url=new_url)
                        return
                await route.continue_()

            await page.route('**/cc.unext.jp/**', modify_request)

            # í˜ì´ì§€ 1: ì¼ë°˜ ë¡œë“œ
            await page.goto(self.url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(8000)
            self.logger.info(f"   Page 1: {len(all_books)}ê°œ ìˆ˜ì§‘")

            # í˜ì´ì§€ 2-5: í˜ì´ì§€ ì¬ë¡œë“œ + route intercept
            for pg in range(2, 6):
                current_page_target[0] = pg
                await page.goto(self.url, wait_until='domcontentloaded', timeout=30000)
                await page.wait_for_timeout(5000)
                self.logger.info(f"   Page {pg}: {len(all_books)}ê°œ ëˆ„ì ")

            # ì¤‘ë³µ ì œê±° ë° ë­í‚¹ êµ¬ì„±
            seen = set()
            rankings = []
            for book in all_books:
                if book['name'] and book['name'] not in seen:
                    seen.add(book['name'])
                    rankings.append({
                        'rank': len(rankings) + 1,
                        'title': book['name'],
                        'genre': '',
                        'url': f"https://video.unext.jp/book/title/{book['code']}" if book['code'] else '',
                        'thumbnail_url': book.get('thumb', ''),
                    })
                    if len(rankings) >= 100:
                        break

            # route í•´ì œ
            await page.unroute('**/cc.unext.jp/**')

            # í´ë°±: GraphQL ì‹¤íŒ¨ ì‹œ DOM ë°©ì‹
            if len(rankings) < 5:
                self.logger.info(f"   GraphQL ë¶€ì¡±({len(rankings)}ê°œ), DOM í´ë°±...")
                for _ in range(15):
                    await page.evaluate('window.scrollBy(0, 800)')
                    await page.wait_for_timeout(500)
                rankings = await self._parse_dom_rankings(page)

            self.genre_results[''] = rankings
            self.logger.info(f"   âœ… [ë§Œí™”]: {len(rankings)}ê°œ ì‘í’ˆ")

            return rankings

        finally:
            await page.close()

    async def _parse_dom_rankings(self, page) -> List[Dict[str, Any]]:
        """DOMì—ì„œ ë­í‚¹ ì•„ì´í…œ + ì¸ë„¤ì¼ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            // U-NEXT: picture > img êµ¬ì¡°, metac.nxtv.jp ë„ë©”ì¸
            // ë­í‚¹ ì•„ì´í…œì€ a íƒœê·¸ë¡œ ê°ì‹¸ì§
            const allLinks = document.querySelectorAll('a[href*="/book/title/"]');
            const seen = new Set();
            let rank = 0;

            for (const a of allLinks) {
                const img = a.querySelector('picture img, img');
                if (!img) continue;

                const src = img.getAttribute('src') || '';
                if (!src || src.includes('logo') || src.includes('icon')) continue;

                // U-NEXT ì´ë¯¸ì§€ ë„ë©”ì¸
                if (!src.includes('nxtv.jp') && !src.includes('unext')) continue;

                const alt = img.getAttribute('alt') || '';
                let title = alt;
                if (!title || title.length < 2) {
                    // ë§í¬ ë‚´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    const textEls = a.querySelectorAll('span, p, h3');
                    for (const el of textEls) {
                        const t = el.textContent.trim();
                        if (t.length >= 2 && t.length <= 100 && !/^\\d+$/.test(t) &&
                            !t.includes('ç„¡æ–™') && !t.includes('New')) {
                            title = t;
                            break;
                        }
                    }
                }
                if (!title || title.length < 2) continue;

                // ì¤‘ë³µ ë°©ì§€
                if (seen.has(title)) continue;
                seen.add(title);

                const href = a.getAttribute('href') || '';
                const fullUrl = href.startsWith('http') ? href : 'https://video.unext.jp' + href;

                rank++;
                if (rank <= 100) {
                    results.push({
                        rank: rank,
                        title: title,
                        url: fullUrl,
                        thumbnail_url: src,
                    });
                }
            }
            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': '',
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    def _parse_text_rankings(self, body_text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ - í´ë°±"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []

        start_idx = 0
        for i, line in enumerate(lines):
            if line == 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°' and i > 10:
                start_idx = i + 1
                break

        i = start_idx
        while i < len(lines):
            if lines[i].isdigit() and 1 <= int(lines[i]) <= 100:
                break
            i += 1

        while i < len(lines) and len(rankings) < 100:
            line = lines[i]

            if line.isdigit() and 1 <= int(line) <= 100:
                rank = int(line)
                j = i + 1
                while j < len(lines):
                    candidate = lines[j].strip()
                    if candidate in ['New', ''] or re.match(r'^\d+å†Šç„¡æ–™$', candidate):
                        j += 1
                        continue
                    break

                if j < len(lines):
                    title = lines[j].strip()
                    if (len(title) >= 2 and
                            not re.match(r'^\d+$', title) and
                            title not in ['ãƒãƒ³ã‚¬', 'ãƒ©ãƒãƒ™', 'æ›¸ç±', 'ãƒ›ãƒ¼ãƒ ']):
                        rankings.append({
                            'rank': rank,
                            'title': title,
                            'genre': '',
                            'url': 'https://video.unext.jp/book/ranking/comic',
                            'thumbnail_url': '',
                        })
                        i = j + 1
                        continue

            i += 1

        return rankings

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ë­í‚¹ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)
