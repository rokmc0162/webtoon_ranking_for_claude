"""
ì´ë¶ì¬íŒ¬ (ebookjapan / Yahoo) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- SSR+CSR í•˜ì´ë¸Œë¦¬ë“œ (Vue.js)
- IP ì œí•œ ì—†ìŒ
- ì¹´í…Œê³ ë¦¬ íƒ­ë³„ ë­í‚¹ (ì´í•©, ì†Œë…€/ì—¬ì„±, ì†Œë…„/ì²­ë…„, íŒíƒ€ì§€, BL, TL ë“±)
- ì´ˆê¸° 10ê°œë§Œ í‘œì‹œ, "ã‚‚ã£ã¨è¦‹ã‚‹" í´ë¦­ìœ¼ë¡œ í™•ì¥
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class EbookjapanAgent(CrawlerAgent):
    """ì´ë¶ì¬íŒ¬ ì¼ê°„ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'path': '/ranking/'},
        'å°‘å¥³ãƒ»å¥³æ€§': {'name': 'ì†Œë…€/ì—¬ì„±', 'path': '/ranking/category/1/'},
        'å°‘å¹´ãƒ»é’å¹´': {'name': 'ì†Œë…„/ì²­ë…„', 'path': '/ranking/category/2/'},
        'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼': {'name': 'íŒíƒ€ì§€', 'path': '/ranking/category/26/'},
        'BL': {'name': 'BL', 'path': '/ranking/category/5/'},
        'TL': {'name': 'TL', 'path': '/ranking/category/4/'},
    }

    def __init__(self):
        super().__init__(
            platform_id='ebookjapan',
            platform_name='ì´ë¶ì¬íŒ¬ (ebookjapan)',
            url='https://ebookjapan.yahoo.co.jp/ranking/'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ì´ë¶ì¬íŒ¬ ì¢…í•© + ì¹´í…Œê³ ë¦¬ë³„ ë­í‚¹ í¬ë¡¤ë§"""
        page = await browser.new_page()
        all_rankings = []

        try:
            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']
                path = genre_info['path']
                url = f'https://ebookjapan.yahoo.co.jp{path}'

                self.logger.info(f"ğŸ“± ì´ë¶ì¬íŒ¬ [{label}] í¬ë¡¤ë§ ì¤‘... â†’ {url}")

                await page.goto(url, wait_until='domcontentloaded', timeout=20000)
                await page.wait_for_timeout(4000)

                # "ã‚‚ã£ã¨è¦‹ã‚‹" ë²„íŠ¼ í´ë¦­ ì‹œë„ (ë” ë§ì€ ì•„ì´í…œ ë¡œë“œ)
                try:
                    more_btn = await page.query_selector('a:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")')
                    if more_btn:
                        await more_btn.click()
                        await page.wait_for_timeout(3000)
                except Exception:
                    pass

                # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹±
                body_text = await page.inner_text('body')
                rankings = self._parse_text_rankings(body_text, genre_key)

                self.genre_results[genre_key] = rankings
                self.logger.info(f"   âœ… [{label}]: {len(rankings)}ê°œ ì‘í’ˆ")

                if genre_key == '':
                    all_rankings = rankings

            return all_rankings

        finally:
            await page.close()

    def _parse_text_rankings(self, body_text: str, genre_key: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []

        # ebookjapanì€ "ì´í•©ãƒ©ãƒ³ã‚­ãƒ³ã‚°" ì´í›„ íƒ€ì´í‹€ + (ê¶Œìˆ˜) + ì¥ë¥´ íŒ¨í„´
        # íƒ€ì´í‹€ í–‰ ë’¤ì— (N) í˜•íƒœì˜ ê¶Œìˆ˜, ê·¸ ë’¤ì— ì¥ë¥´ í‘œê¸°
        in_ranking = False
        rank = 0

        for i, line in enumerate(lines):
            if 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°' in line and ('ç·åˆ' in line or 'å°‘å¥³' in line or
                                        'å°‘å¹´' in line or 'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼' in line):
                in_ranking = True
                continue

            if not in_ranking:
                continue

            # "ã‚‚ã£ã¨è¦‹ã‚‹" ì´í›„ ìƒˆ ì„¹ì…˜
            if line == 'ã‚‚ã£ã¨è¦‹ã‚‹':
                continue

            # ì¥ë¥´ íƒœê·¸ (ìŠ¤í‚µ)
            if line in ['å°‘å¥³ãƒãƒ³ã‚¬', 'å¥³æ€§ãƒãƒ³ã‚¬', 'é’å¹´ãƒãƒ³ã‚¬', 'å°‘å¹´ãƒãƒ³ã‚¬',
                         'BLã‚³ãƒŸãƒƒã‚¯', 'TLã‚³ãƒŸãƒƒã‚¯', 'ãƒ©ãƒãƒ™']:
                continue

            # ê¶Œìˆ˜ í‘œê¸° (ìŠ¤í‚µ)
            if re.match(r'^ï¼ˆ\d+ï¼‰$', line):
                continue

            # ì§§ì€ ìœ í‹¸ë¦¬í‹° í…ìŠ¤íŠ¸ ìŠ¤í‚µ
            if len(line) < 3 or line in ['æœ‰æ–™', 'ç„¡æ–™', 'å‰æ—¥', 'é€±é–“', 'æœˆé–“',
                                          'æ­´ä»£', 'ãƒˆãƒƒãƒ—', 'ç·åˆ']:
                continue

            # íƒ€ì´í‹€ í›„ë³´
            if len(line) >= 3 and not line.startswith('http'):
                rank += 1
                if rank <= 100:
                    # ë‹¤ìŒ ì¤„ì—ì„œ ì¥ë¥´ ì¶”ì¶œ ì‹œë„
                    genre = genre_key
                    if i + 2 < len(lines):
                        next_next = lines[i + 2].strip() if i + 2 < len(lines) else ''
                        if next_next in ['å°‘å¥³ãƒãƒ³ã‚¬', 'å¥³æ€§ãƒãƒ³ã‚¬', 'é’å¹´ãƒãƒ³ã‚¬',
                                         'å°‘å¹´ãƒãƒ³ã‚¬']:
                            genre = next_next

                    rankings.append({
                        'rank': rank,
                        'title': line,
                        'genre': genre,
                        'url': 'https://ebookjapan.yahoo.co.jp/ranking/',
                        'thumbnail_url': '',
                    })

        return rankings[:100]

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('title')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
