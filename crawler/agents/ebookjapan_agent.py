"""
ì´ë¶ìž¬íŒ¬ (ebookjapan / Yahoo) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

URL êµ¬ì¡° (2026ë…„ ê¸°ì¤€):
- ì¢…í•© ëž­í‚¹: /ranking/details/?page=1, ?page=2  (ê° 50ìœ„, ì´ 100ìœ„)
- ì†Œë…€/ì—¬ì„±: /ranking/details/?genre=womens&page=1, &page=2
- ì†Œë…„/ì²­ë…„: /ranking/details/?genre=mens&page=1, &page=2
- íŒíƒ€ì§€:    /ranking/details/?tag=112&page=1, &page=2
- BL:       /ranking/details/?genre=bl&page=1, &page=2
- TL:       /ranking/details/?genre=tl&page=1, &page=2

ì£¼ì˜: /ranking/ ìµœìƒìœ„ íŽ˜ì´ì§€ëŠ” ì¹´í…Œê³ ë¦¬ë³„ 3ê°œë§Œ í‘œì‹œí•˜ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class EbookjapanAgent(CrawlerAgent):
    """ì´ë¶ìž¬íŒ¬ ì¼ê°„ ëž­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/'},
        'å°‘å¥³ãƒ»å¥³æ€§': {'name': 'ì†Œë…€/ì—¬ì„±', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=womens'},
        'å°‘å¹´ãƒ»é’å¹´': {'name': 'ì†Œë…„/ì²­ë…„', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=mens'},
        'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼': {'name': 'íŒíƒ€ì§€', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?tag=112'},
        'BL': {'name': 'BL', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=bl'},
        'TL': {'name': 'TL', 'base_url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=tl'},
    }

    def __init__(self):
        super().__init__(
            platform_id='ebookjapan',
            platform_name='ì´ë¶ìž¬íŒ¬ (ebookjapan)',
            url='https://ebookjapan.yahoo.co.jp/ranking/details/'
        )
        self.genre_results = {}

    def _build_page_url(self, base_url: str, page: int) -> str:
        """íŽ˜ì´ì§€ URL ìƒì„±"""
        if '?' in base_url:
            return f"{base_url}&page={page}"
        else:
            return f"{base_url}?page={page}"

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ì´ë¶ìž¬íŒ¬ ì¢…í•© + ì¹´í…Œê³ ë¦¬ë³„ ëž­í‚¹ í¬ë¡¤ë§"""
        page = await browser.new_page()
        all_rankings = []

        try:
            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']
                base_url = genre_info['base_url']

                self.logger.info(f"ðŸ“± ì´ë¶ìž¬íŒ¬ [{label}] í¬ë¡¤ë§ ì¤‘...")

                try:
                    genre_rankings = []

                    # 2íŽ˜ì´ì§€ ìˆœíšŒ (ê° 50ìœ„ì”©, ì´ 100ìœ„)
                    for page_num in [1, 2]:
                        page_url = self._build_page_url(base_url, page_num)
                        rank_offset = (page_num - 1) * 50

                        self.logger.info(f"   íŽ˜ì´ì§€ {page_num}: {page_url}")

                        await page.goto(page_url, wait_until='domcontentloaded', timeout=20000)
                        await page.wait_for_timeout(3000)

                        # íŒì—… ë‹«ê¸°
                        try:
                            close_btn = await page.query_selector('button:has-text("é–‰ã˜ã‚‹")')
                            if close_btn:
                                await close_btn.click()
                                await page.wait_for_timeout(1000)
                        except Exception:
                            pass

                        # ìŠ¤í¬ë¡¤ë¡œ lazy loading íŠ¸ë¦¬ê±°
                        for scroll_i in range(12):
                            await page.evaluate('window.scrollBy(0, 800)')
                            await page.wait_for_timeout(400)

                        # DOM íŒŒì‹±
                        items = await self._parse_page_rankings(page, genre_key, rank_offset)
                        self.logger.info(f"   íŽ˜ì´ì§€ {page_num}: {len(items)}ê°œ ì¶”ì¶œ")
                        genre_rankings.extend(items)

                    self.logger.info(f"   âœ… [{label}]: {len(genre_rankings)}ê°œ ìž‘í’ˆ")

                    self.genre_results[genre_key] = genre_rankings
                    if genre_key == '':
                        all_rankings = genre_rankings

                except Exception as e:
                    self.logger.warning(f"   âš ï¸ [{label}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    self.genre_results[genre_key] = []
                    continue

            return all_rankings

        finally:
            await page.close()

    async def _parse_page_rankings(self, page, genre_key: str, rank_offset: int) -> List[Dict[str, Any]]:
        """í•œ íŽ˜ì´ì§€(50ê°œ)ì—ì„œ ëž­í‚¹ ì•„ì´í…œ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            const imgs = document.querySelectorAll('img.cover-main__img');
            const seenTitles = new Set();

            for (const img of imgs) {
                const src = img.getAttribute('src') || '';
                if (!src.startsWith('http') || src.includes('loading-book-cover')) continue;

                let alt = img.getAttribute('alt') || '';
                if (!alt || alt.length < 2) continue;

                // ê¶Œìˆ˜/ë ˆì´ë¸” íŒ¨í„´ ì œê±°
                let title = alt
                    .replace(/[\\sã€€]+ï¼ˆ[ï¼-ï¼™0-9]+ï¼‰ï¼ˆ[^ï¼‰]+ï¼‰$/u, '')  // ï¼ˆ8ï¼‰ï¼ˆã‚¬ãƒ«ãƒ‰ã‚³ãƒŸãƒƒã‚¯ã‚¹ï¼‰
                    .replace(/[\\sã€€]*ï¼ˆ[ï¼-ï¼™0-9]+ï¼‰$/u, '')           // ï¼ˆ8ï¼‰
                    .replace(/[\\sã€€]+[ï¼-ï¼™]+$/u, '')                  // ì „ê° ìˆ«ìž
                    .replace(/[\\sã€€]+\\d+å·»?$/, '')                    // Nå·» or N
                    .replace(/[\\sã€€]*[:ï¼š][\\sã€€]*\\d*$/, '')          // ï¼šN or trailing ï¼š
                    .replace(/\\d+ã€[^ã€‘]*ã€‘$/, '')                     // 16ã€é›»å­ç‰¹å…¸ä»˜ãã€‘
                    .replace(/ã€[^ã€‘]*ã€‘$/, '')                         // ã€é›»å­é™å®šç‰¹å…¸ä»˜ãã€‘
                    .replace(/[\\sã€€]+\\d+å·.*$/, '')                   // 13å· [2026å¹´...]
                    .replace(/\\s*\\[\\d{4}å¹´.*$/, '')                   // [2026å¹´2æœˆ25æ—¥ç™ºå£²]
                    .trim();
                if (!title || title.length < 2) continue;

                if (seenTitles.has(title)) continue;
                seenTitles.add(title);

                // URL ì¶”ì¶œ
                const container = img.closest('li') || img.closest('div') || img.parentElement;
                const linkEl = container ? container.querySelector('a[href*="/books/"]') || container.querySelector('a') : null;
                const href = linkEl ? linkEl.getAttribute('href') : '';
                const fullUrl = href ? (href.startsWith('http') ? href : 'https://ebookjapan.yahoo.co.jp' + href) : '';

                results.push({
                    title: title,
                    url: fullUrl,
                    thumbnail_url: src,
                });
            }
            return results;
        }""")

        rankings = []
        for i, item in enumerate(items[:50]):
            rankings.append({
                'rank': rank_offset + i + 1,
                'title': item['title'],
                'genre': genre_key,
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            })

        return rankings

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ìž¥ë¥´ë³„ ëž­í‚¹ ëª¨ë‘ ì €ìž¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            genre_meta = [
                {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
                 'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
                for item in rankings if item.get('thumbnail_url')
            ]
            if genre_meta:
                save_works_metadata(self.platform_id, genre_meta, date=date, sub_category=genre_key)
            self.logger.info(f"   ðŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ìž¥")
