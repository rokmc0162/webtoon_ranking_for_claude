"""
ì´ë¶ì¬íŒ¬ (ebookjapan / Yahoo) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- SSR+CSR í•˜ì´ë¸Œë¦¬ë“œ (Vue.js)
- IP ì œí•œ ì—†ìŒ
- img.cover-main__img ì…€ë ‰í„°ë¡œ ì¸ë„¤ì¼ ì¶”ì¶œ
- ì¢…í•©: /ranking/ í˜ì´ì§€ì—ì„œ "ã‚‚ã£ã¨è¦‹ã‚‹" í´ë¦­ + ìŠ¤í¬ë¡¤
- ì¥ë¥´ë³„: /ranking/details/?genre=xxx ë˜ëŠ” ?tag=xxx (2026 URL íŒ¨í„´)
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class EbookjapanAgent(CrawlerAgent):
    """ì´ë¶ì¬íŒ¬ ì¼ê°„ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/'},
        'å°‘å¥³ãƒ»å¥³æ€§': {'name': 'ì†Œë…€/ì—¬ì„±', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=womens'},
        'å°‘å¹´ãƒ»é’å¹´': {'name': 'ì†Œë…„/ì²­ë…„', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=mens'},
        'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼': {'name': 'íŒíƒ€ì§€', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?tag=112'},
        'BL': {'name': 'BL', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=bl'},
        'TL': {'name': 'TL', 'url': 'https://ebookjapan.yahoo.co.jp/ranking/details/?genre=tl'},
    }

    def __init__(self):
        super().__init__(
            platform_id='ebookjapan',
            platform_name='ì´ë¶ì¬íŒ¬ (ebookjapan)',
            url='https://ebookjapan.yahoo.co.jp/ranking/'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ì´ë¶ì¬íŒ¬ ì¢…í•© + ì¹´í…Œê³ ë¦¬ë³„ ë­í‚¹ í¬ë¡¤ë§ - DOM ê¸°ë°˜"""
        page = await browser.new_page()
        all_rankings = []

        try:
            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']
                url = genre_info['url']

                self.logger.info(f"ğŸ“± ì´ë¶ì¬íŒ¬ [{label}] í¬ë¡¤ë§ ì¤‘... â†’ {url}")

                await page.goto(url, wait_until='domcontentloaded', timeout=20000)
                await page.wait_for_timeout(4000)

                # íŒì—… ë‹«ê¸° (ì¿ í°/ë¯¸ì…˜ íŒì—…)
                try:
                    close_btn = await page.query_selector('button:has-text("é–‰ã˜ã‚‹")')
                    if close_btn:
                        await close_btn.click()
                        await page.wait_for_timeout(1000)
                except Exception:
                    pass

                # ì¢…í•© í˜ì´ì§€: "ã‚‚ã£ã¨è¦‹ã‚‹" í´ë¦­ + ëŒ€ëŸ‰ ìŠ¤í¬ë¡¤ë¡œ 100ê°œ ë¡œë“œ
                if genre_key == '':
                    try:
                        more_btn = await page.query_selector('a:has-text("ã‚‚ã£ã¨è¦‹ã‚‹")')
                        if more_btn:
                            await more_btn.click()
                            await page.wait_for_timeout(3000)
                            self.logger.info(f"   ã‚‚ã£ã¨è¦‹ã‚‹ í´ë¦­ ì™„ë£Œ")
                    except Exception:
                        pass

                    # ëŒ€ëŸ‰ ìŠ¤í¬ë¡¤ë¡œ lazy loading íŠ¸ë¦¬ê±° (100ê°œ ì´ìƒ ë¡œë“œ)
                    prev_count = 0
                    for scroll_i in range(25):
                        await page.evaluate('window.scrollBy(0, 1000)')
                        await page.wait_for_timeout(500)
                        if scroll_i % 5 == 4:
                            curr_count = await page.evaluate("""() => {
                                const imgs = document.querySelectorAll('img.cover-main__img');
                                let c = 0;
                                for (const img of imgs) {
                                    const s = img.getAttribute('src') || '';
                                    if (s.startsWith('http') && !s.includes('loading')) c++;
                                }
                                return c;
                            }""")
                            self.logger.info(f"   ìŠ¤í¬ë¡¤ {scroll_i+1}: {curr_count}ê°œ ë¡œë“œë¨")
                            if curr_count >= 100 or (curr_count == prev_count and curr_count > 0):
                                break
                            prev_count = curr_count
                else:
                    # ì¥ë¥´ í˜ì´ì§€ (/ranking/details/): ìŠ¤í¬ë¡¤ë¡œ 50ê°œ ë¡œë“œ
                    prev_count = 0
                    for scroll_i in range(15):
                        await page.evaluate('window.scrollBy(0, 800)')
                        await page.wait_for_timeout(500)
                        if scroll_i % 5 == 4:
                            curr_count = await page.evaluate("""() => {
                                const imgs = document.querySelectorAll('img.cover-main__img');
                                let c = 0;
                                for (const img of imgs) {
                                    const s = img.getAttribute('src') || '';
                                    if (s.startsWith('http') && !s.includes('loading')) c++;
                                }
                                return c;
                            }""")
                            self.logger.info(f"   ìŠ¤í¬ë¡¤ {scroll_i+1}: {curr_count}ê°œ ë¡œë“œë¨")
                            if curr_count >= 50 or (curr_count == prev_count and curr_count > 0):
                                break
                            prev_count = curr_count

                # DOM íŒŒì‹±ì„ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš© (img altì—ì„œ íƒ€ì´í‹€ + ì¸ë„¤ì¼)
                rankings = await self._parse_dom_rankings(page, genre_key)

                # DOM ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ í…ìŠ¤íŠ¸ íŒŒì‹± í´ë°± (ì¢…í•©ë§Œ)
                if len(rankings) < 5 and genre_key == '':
                    self.logger.info(f"   DOM ë¶€ì¡±({len(rankings)}ê°œ), í…ìŠ¤íŠ¸ í´ë°±...")
                    body_text = await page.inner_text('body')
                    rankings = self._parse_text_rankings(body_text, genre_key)

                self.logger.info(f"   âœ… [{label}]: {len(rankings)}ê°œ ì‘í’ˆ")

                self.genre_results[genre_key] = rankings
                if genre_key == '':
                    all_rankings = rankings

            return all_rankings

        finally:
            await page.close()

    async def _parse_dom_rankings(self, page, genre_key: str) -> List[Dict[str, Any]]:
        """DOMì—ì„œ ë­í‚¹ ì•„ì´í…œ + ì¸ë„¤ì¼ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            // ebookjapan: img.cover-main__img (lazy-img í¬í•¨)
            // srcê°€ cache2-ebookjapan ë„ë©”ì¸ì´ë©´ ì‹¤ì œ ì¸ë„¤ì¼, loading-book-cover.svgë©´ ë¯¸ë¡œë”©
            const imgs = document.querySelectorAll('img.cover-main__img');
            let rank = 0;
            const seenTitles = new Set();

            for (const img of imgs) {
                const src = img.getAttribute('src') || '';
                // ì‹¤ì œ ì¸ë„¤ì¼ URLë§Œ (lazy loading placeholder ìŠ¤í‚µ)
                if (!src.startsWith('http') || src.includes('loading-book-cover')) continue;

                // íƒ€ì´í‹€: altì—ì„œ ê¶Œìˆ˜ ì œê±°
                let alt = img.getAttribute('alt') || '';
                if (!alt || alt.length < 2) continue;

                // ê¶Œìˆ˜ íŒ¨í„´ ì œê±°: "ã‚¿ã‚¤ãƒˆãƒ«ã€€ï¼‘ï¼‘" â†’ "ã‚¿ã‚¤ãƒˆãƒ«"
                // íŒ¨í„´: ì „ê°ìˆ«ì, ë°˜ê°ìˆ«ì + å·», ë°˜ê°ìˆ«ìë§Œ
                let title = alt
                    .replace(/[\\sã€€]+[ï¼-ï¼™]+$/, '')           // ì „ê° ìˆ«ì
                    .replace(/[\\sã€€]+\\d+å·»?$/, '')             // Nå·» or N
                    .replace(/[\\sã€€]*[:ï¼š][\\sã€€]*\\d+$/, '')    // ï¼š N
                    .trim();
                if (!title || title.length < 2) continue;

                // ì¤‘ë³µ ì œê±°
                if (seenTitles.has(title)) continue;
                seenTitles.add(title);

                // ê°€ì¥ ê°€ê¹Œìš´ a íƒœê·¸ì—ì„œ URL ì¶”ì¶œ
                const container = img.closest('li') || img.closest('div') || img.parentElement;
                const linkEl = container ? container.querySelector('a[href*="/books/"]') || container.querySelector('a') : null;
                const href = linkEl ? linkEl.getAttribute('href') : '';
                const fullUrl = href ? (href.startsWith('http') ? href : 'https://ebookjapan.yahoo.co.jp' + href) : '';

                rank++;
                if (rank <= 100) {
                    results.push({
                        rank: rank,
                        title: title,
                        url: fullUrl,
                        thumbnail_url: src,
                    });
                }
            }
            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': genre_key,
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    def _parse_text_rankings(self, body_text: str, genre_key: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ (í´ë°±)"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []
        in_ranking = False
        rank = 0

        for i, line in enumerate(lines):
            if 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°' in line and ('ç·åˆ' in line or 'å°‘å¥³' in line or
                                        'å°‘å¹´' in line or 'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼' in line):
                in_ranking = True
                continue
            if not in_ranking:
                continue
            if line == 'ã‚‚ã£ã¨è¦‹ã‚‹':
                continue
            if line in ['å°‘å¥³ãƒãƒ³ã‚¬', 'å¥³æ€§ãƒãƒ³ã‚¬', 'é’å¹´ãƒãƒ³ã‚¬', 'å°‘å¹´ãƒãƒ³ã‚¬',
                         'BLã‚³ãƒŸãƒƒã‚¯', 'TLã‚³ãƒŸãƒƒã‚¯', 'ãƒ©ãƒãƒ™']:
                continue
            if re.match(r'^ï¼ˆ\d+ï¼‰$', line):
                continue
            if len(line) < 3 or line in ['æœ‰æ–™', 'ç„¡æ–™', 'å‰æ—¥', 'é€±é–“', 'æœˆé–“',
                                          'æ­´ä»£', 'ãƒˆãƒƒãƒ—', 'ç·åˆ']:
                continue
            if len(line) >= 3 and not line.startswith('http'):
                rank += 1
                if rank <= 100:
                    rankings.append({
                        'rank': rank,
                        'title': line,
                        'genre': genre_key,
                        'url': 'https://ebookjapan.yahoo.co.jp/ranking/',
                        'thumbnail_url': '',
                    })

        return rankings[:100]

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            genre_meta = [
                {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
                 'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
                for item in rankings if item.get('thumbnail_url')
            ]
            if genre_meta:
                save_works_metadata(self.platform_id, genre_meta, date=date, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
