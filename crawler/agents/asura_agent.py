"""
Asura Scans (asuracomic.net) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- ì˜ì–´ ë²ˆì—­ í•´ì íŒ ì›¹íˆ° ì‚¬ì´íŠ¸ (í•œêµ­ ì›¹íˆ° ì¤‘ì‹¬)
- SSR ë°©ì‹ â†’ HTTP ìš”ì²­ìœ¼ë¡œë„ ê°€ëŠ¥í•˜ë‚˜, Playwrightë¡œ í†µì¼
- IP ì œí•œ ì—†ìŒ
- ì•½ 350~400ê°œ ì‹œë¦¬ì¦ˆ
- ìˆ˜ì§‘ ë°ì´í„°:
  1. ì¸ê¸° ë­í‚¹ (Weekly/Monthly/All TOP 10)
  2. ì‹œë¦¬ì¦ˆ ëª©ë¡ (ì¸ê¸°ìˆœ, ~370ê°œ)
  3. ì‘í’ˆ ìƒì„¸ (ë³„ì , íŒ”ë¡œì›Œ, ì¥ë¥´, ì±•í„° ìˆ˜, ëŒ“ê¸€ ìˆ˜)
  4. ëŒ“ê¸€ (ì‘í’ˆë³„)

ì£¼ì˜:
- í•´ì íŒ ì‚¬ì´íŠ¸ â†’ ë„ë©”ì¸ ë³€ê²½/íì‡„ ê°€ëŠ¥ì„±
- ê¸°ì¡´ ì¼ë³¸ í”Œë«í¼ í¬ë¡¤ë§ê³¼ ë¶„ë¦¬ ì‹¤í–‰
- ìš”ì²­ ê°„ê²© ì¶©ë¶„íˆ ë‘ê¸° (3~5ì´ˆ)
"""

import re
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from playwright.async_api import Browser, Page

logger = logging.getLogger('crawler.agents.asura')

BASE_URL = 'https://asuracomic.net'


class AsuraAgent:
    """Asura Scans í¬ë¡¤ëŸ¬ (ë…ë¦½ ì‹¤í–‰)"""

    def __init__(self):
        self.platform_id = 'asura'
        self.platform_name = 'Asura Scans'
        self.logger = logger
        self.results = {
            'rankings_weekly': [],
            'rankings_monthly': [],
            'rankings_all': [],
            'series_list': [],       # ì¸ê¸°ìˆœ ì „ì²´ ëª©ë¡
            'series_details': [],    # ìƒì„¸ ë©”íƒ€ë°ì´í„°
            'comments': [],          # ëŒ“ê¸€
        }

    async def execute(self, browser: Browser,
                      phases: List[str] = None) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ ì‹¤í–‰

        Args:
            browser: Playwright ë¸Œë¼ìš°ì €
            phases: ì‹¤í–‰í•  í˜ì´ì¦ˆ ['rankings', 'series', 'details', 'comments']
                    Noneì´ë©´ ì „ë¶€ ì‹¤í–‰

        Returns:
            {phase: count} ê²°ê³¼ ìš”ì•½
        """
        if phases is None:
            phases = ['rankings', 'series', 'details', 'comments']

        ctx = await browser.new_context(
            locale='en-US',
            viewport={'width': 1366, 'height': 768},
            ignore_https_errors=True,
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/120.0.0.0 Safari/537.36',
        )
        page = await ctx.new_page()
        summary = {}

        try:
            # Phase 1: ì¸ê¸° ë­í‚¹ (Weekly/Monthly/All TOP 10)
            if 'rankings' in phases:
                self.logger.info("ğŸ“Š [Phase 1] ì¸ê¸° ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘...")
                await self._crawl_rankings(page)
                summary['rankings'] = {
                    'weekly': len(self.results['rankings_weekly']),
                    'monthly': len(self.results['rankings_monthly']),
                    'all': len(self.results['rankings_all']),
                }
                self.logger.info(
                    f"   âœ… Weekly {summary['rankings']['weekly']}ê°œ, "
                    f"Monthly {summary['rankings']['monthly']}ê°œ, "
                    f"All {summary['rankings']['all']}ê°œ"
                )

            # Phase 2: ì‹œë¦¬ì¦ˆ ì „ì²´ ëª©ë¡ (ì¸ê¸°ìˆœ)
            if 'series' in phases:
                self.logger.info("ğŸ“š [Phase 2] ì‹œë¦¬ì¦ˆ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")
                await self._crawl_series_list(page)
                summary['series'] = len(self.results['series_list'])
                self.logger.info(f"   âœ… {summary['series']}ê°œ ì‹œë¦¬ì¦ˆ")

            # Phase 3: ì‘í’ˆ ìƒì„¸ + ëŒ“ê¸€
            if 'details' in phases or 'comments' in phases:
                targets = self.results['series_list']
                if not targets:
                    self.logger.warning("   âš ï¸ ì‹œë¦¬ì¦ˆ ëª©ë¡ì´ ë¹„ì–´ìˆìŒ - Phase 2 ë¨¼ì € ì‹¤í–‰ í•„ìš”")
                else:
                    self.logger.info(
                        f"ğŸ“ [Phase 3] ì‘í’ˆ ìƒì„¸ + ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘... "
                        f"({len(targets)}ê°œ ì‘í’ˆ)"
                    )
                    collect_comments = 'comments' in phases
                    await self._crawl_details(page, targets, collect_comments)
                    summary['details'] = len(self.results['series_details'])
                    summary['comments'] = len(self.results['comments'])
                    self.logger.info(
                        f"   âœ… ìƒì„¸ {summary.get('details', 0)}ê°œ, "
                        f"ëŒ“ê¸€ {summary.get('comments', 0)}ê°œ"
                    )

            return summary

        except Exception as e:
            self.logger.error(f"âŒ Asura í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
        finally:
            await page.close()
            await ctx.close()

    # ===== Phase 1: ì¸ê¸° ë­í‚¹ =====

    async def _crawl_rankings(self, page: Page):
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ Weekly/Monthly/All ì¸ê¸° ë­í‚¹ ìˆ˜ì§‘"""
        await page.goto(BASE_URL, wait_until='domcontentloaded', timeout=30000)
        await page.wait_for_timeout(5000)

        # íƒ­ êµ¬ì¡°: button[data-state=active/inactive] â†’ role="tabpanel"[data-state]
        # Weekly (ê¸°ë³¸ í™œì„± íƒ­)
        self.results['rankings_weekly'] = await self._extract_popular_tab(
            page, 'weekly'
        )

        # Monthly íƒ­ í´ë¦­
        monthly_tab = await page.query_selector('button:has-text("Monthly")')
        if monthly_tab:
            await monthly_tab.click()
            await page.wait_for_timeout(2000)
            self.results['rankings_monthly'] = await self._extract_popular_tab(
                page, 'monthly'
            )

        # All íƒ­ í´ë¦­
        all_tab = await page.query_selector('button:has-text("All")')
        if all_tab:
            await all_tab.click()
            await page.wait_for_timeout(2000)
            self.results['rankings_all'] = await self._extract_popular_tab(
                page, 'all'
            )

    async def _extract_popular_tab(self, page: Page, period: str) -> List[Dict]:
        """í˜„ì¬ í™œì„± íƒ­íŒ¨ë„ì—ì„œ ë­í‚¹ ì¶”ì¶œ

        DOM êµ¬ì¡°:
        - role="tabpanel"[data-state="active"] ì•ˆì— ì‹œë¦¬ì¦ˆ ë§í¬ë“¤
        - ê° ì‹œë¦¬ì¦ˆ: ì´ë¯¸ì§€ ë§í¬(ë¹ˆ í…ìŠ¤íŠ¸) + í…ìŠ¤íŠ¸ ë§í¬(ì œëª©) ìŒ
        - í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ë§Œ í•„í„°í•˜ì—¬ ì¶”ì¶œ
        """
        items = await page.evaluate("""() => {
            const results = [];

            // í™œì„± íƒ­íŒ¨ë„ ì°¾ê¸°
            const panel = document.querySelector(
                '[role="tabpanel"][data-state="active"]'
            );
            if (!panel) return results;

            // ì‹œë¦¬ì¦ˆ ë§í¬ ì¤‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë§Œ
            const links = panel.querySelectorAll('a[href*="/series/"]');
            const seen = new Set();
            let rank = 0;

            for (const link of links) {
                const href = link.getAttribute('href') || '';
                if (!href.match(/\\/series\\/[a-z]/)) continue;
                if (seen.has(href)) continue;

                const text = link.innerText.trim();
                if (!text || text.length < 2) continue;  // ì´ë¯¸ì§€ ë§í¬ ê±´ë„ˆë›°ê¸°

                // ë¶€ëª¨ì—ì„œ ë³„ì  ì°¾ê¸°
                const parent = link.parentElement;
                const parentText = parent ? parent.textContent : '';
                const ratingMatch = parentText.match(/(\\d+\\.\\d+)/);
                const rating = ratingMatch ? parseFloat(ratingMatch[1]) : null;

                // ì¸ë„¤ì¼ (ê°™ì€ hrefì˜ ì´ì „ í˜•ì œ ì´ë¯¸ì§€ ë§í¬ì—ì„œ)
                let thumbUrl = '';
                const allSiblingLinks = parent ?
                    parent.querySelectorAll('a[href*="/series/"]') : [];
                for (const sib of allSiblingLinks) {
                    if (sib.getAttribute('href') === href) {
                        const img = sib.querySelector('img');
                        if (img) {
                            thumbUrl = img.getAttribute('src') || '';
                            break;
                        }
                    }
                }

                seen.add(href);
                rank++;

                results.push({
                    rank: rank,
                    title: text.replace(/\\.\\.\\.$/, '').trim(),
                    rating: rating,
                    url: href.startsWith('http') ? href :
                        'https://asuracomic.net' + href,
                    thumbnail_url: thumbUrl,
                });
            }

            return results.slice(0, 10);
        }""")

        return [
            {**item, 'period': period}
            for item in items
        ]

    # ===== Phase 2: ì‹œë¦¬ì¦ˆ ì „ì²´ ëª©ë¡ =====

    async def _crawl_series_list(self, page: Page):
        """ì „ì²´ ì‹œë¦¬ì¦ˆë¥¼ ì¸ê¸°ìˆœìœ¼ë¡œ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜)

        DOM êµ¬ì¡°:
        - ë©”ì¸ ê·¸ë¦¬ë“œ ì¹´ë“œ: a[href^="series/"] (ìŠ¬ë˜ì‹œ ì—†ì´ ì‹œì‘)
        - ì‚¬ì´ë“œë°” Popular: a[href^="/series/"] (ìŠ¬ë˜ì‹œ ìˆìŒ)
        - ì¹´ë“œ innerText: "STATUS | TYPE | Title | Chapter N | Rating"
        - 15ê°œ/í˜ì´ì§€
        """
        all_series = []
        page_num = 1

        while True:
            url = f'{BASE_URL}/series?page={page_num}&order=popular'
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(3000)

            try:
                await page.wait_for_load_state('domcontentloaded')
            except Exception:
                pass

            items = await page.evaluate("""() => {
                const results = [];
                // ë©”ì¸ ê·¸ë¦¬ë“œ ì¹´ë“œ: hrefê°€ "series/"ë¡œ ì‹œì‘ (/ ì—†ì´)
                // ì‚¬ì´ë“œë°”ëŠ” "/series/"ë¡œ ì‹œì‘ â†’ êµ¬ë¶„ ê°€ëŠ¥
                const allLinks = document.querySelectorAll('a');

                for (const link of allLinks) {
                    const href = link.getAttribute('href') || '';
                    // ë©”ì¸ ê·¸ë¦¬ë“œë§Œ: series/ë¡œ ì‹œì‘ (ì•ì— / ì—†ìŒ)
                    if (!href.match(/^series\\/[a-z]/)) continue;

                    const text = link.innerText || '';
                    const parts = text.split('\\n').map(s => s.trim()).filter(Boolean);
                    // parts ì˜ˆ: ["ONGOING", "MANHWA", "Title", "Chapter 39", "7.0"]

                    if (parts.length < 3) continue;

                    // ìƒíƒœ (ì²« ë²ˆì§¸ íŒŒíŠ¸)
                    let status = 'Unknown';
                    const statusMap = {
                        'ONGOING': 'Ongoing', 'DROPPED': 'Dropped',
                        'HIATUS': 'Hiatus', 'SEASON END': 'Season End',
                        'COMPLETED': 'Completed',
                    };
                    if (statusMap[parts[0]]) {
                        status = statusMap[parts[0]];
                    }

                    // íƒ€ì… (ë‘ ë²ˆì§¸ íŒŒíŠ¸)
                    let type = 'MANHWA';
                    if (parts.length > 1) {
                        const t = parts[1].toUpperCase();
                        if (t === 'MANGA' || t === 'MANGATOON') type = 'MANGA';
                        else if (t === 'MANHUA') type = 'MANHUA';
                        else if (t === 'MANHWA') type = 'MANHWA';
                    }

                    // ì œëª© (ìƒíƒœ/íƒ€ì… ì´í›„, Chapter ì´ì „)
                    let title = '';
                    let latestChapter = null;
                    let rating = null;

                    for (let i = 2; i < parts.length; i++) {
                        const p = parts[i];
                        const chMatch = p.match(/^Chapter\\s*(\\d+)/i);
                        if (chMatch) {
                            latestChapter = parseInt(chMatch[1]);
                            continue;
                        }
                        const rMatch = p.match(/^(\\d+\\.\\d+)$/);
                        if (rMatch) {
                            rating = parseFloat(rMatch[1]);
                            continue;
                        }
                        if (!title) title = p;
                    }

                    if (!title || title.length < 2) continue;

                    // ì¸ë„¤ì¼
                    const img = link.querySelector('img');
                    const thumbUrl = img ?
                        (img.getAttribute('src') || '') : '';

                    const fullUrl = 'https://asuracomic.net/' + href;

                    results.push({
                        title: title,
                        rating: rating,
                        status: status,
                        latest_chapter: latestChapter,
                        type: type,
                        url: fullUrl,
                        thumbnail_url: thumbUrl,
                    });
                }
                return results;
            }""")

            if not items:
                self.logger.info(
                    f"   í˜ì´ì§€ {page_num}: ì‹œë¦¬ì¦ˆ ì—†ìŒ, ì¤‘ë‹¨"
                )
                break

            # ì¤‘ë³µ ì œê±° (ì´ì „ í˜ì´ì§€ì™€)
            existing_urls = {s['url'] for s in all_series}
            new_items = [
                item for item in items
                if item['url'] not in existing_urls
            ]

            if not new_items:
                self.logger.info(
                    f"   í˜ì´ì§€ {page_num}: ìƒˆ ì‹œë¦¬ì¦ˆ ì—†ìŒ, ì¤‘ë‹¨"
                )
                break

            all_series.extend(new_items)
            self.logger.info(
                f"   í˜ì´ì§€ {page_num}: {len(new_items)}ê°œ "
                f"(ëˆ„ì  {len(all_series)}ê°œ)"
            )

            page_num += 1
            if page_num > 30:  # ì•ˆì „ ì œí•œ
                break

        # ì¸ê¸°ìˆœ ë­í¬ ë¶€ì—¬
        for i, series in enumerate(all_series, 1):
            series['rank'] = i

        self.results['series_list'] = all_series

    # ===== Phase 3: ì‘í’ˆ ìƒì„¸ + ëŒ“ê¸€ =====

    async def _crawl_details(self, page: Page,
                             targets: List[Dict],
                             collect_comments: bool = True):
        """ê° ì‘í’ˆì˜ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        total = len(targets)

        for idx, series in enumerate(targets, 1):
            url = series['url']
            title = series['title']

            try:
                self.logger.info(
                    f"   [{idx}/{total}] {title[:30]}..."
                )

                await page.goto(
                    url, wait_until='domcontentloaded', timeout=30000
                )
                await page.wait_for_timeout(2000)

                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                detail = await self._extract_detail(page, url)
                if detail:
                    detail['title'] = title
                    detail['url'] = url
                    self.results['series_details'].append(detail)

                # ëŒ“ê¸€ ìˆ˜ì§‘
                if collect_comments:
                    comments = await self._extract_comments(page, title)
                    if comments:
                        self.results['comments'].extend(comments)
                        self.logger.info(
                            f"      ğŸ’¬ ëŒ“ê¸€ {len(comments)}ê°œ"
                        )

                # ìš”ì²­ ê°„ê²© (í•´ì íŒ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ë„‰ë„‰íˆ)
                await page.wait_for_timeout(2000)

            except Exception as e:
                self.logger.warning(
                    f"      âš ï¸ {title[:30]} ìƒì„¸ ì‹¤íŒ¨: {e}"
                )

    async def _extract_detail(self, page: Page,
                              url: str) -> Optional[Dict]:
        """ì‘í’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        detail = await page.evaluate("""() => {
            const result = {};
            const body = document.body.textContent || '';

            // ë³„ì 
            const ratingEls = document.querySelectorAll(
                'span, p, div'
            );
            for (const el of ratingEls) {
                const t = el.textContent.trim();
                const m = t.match(/^(\\d+\\.\\d+)$/);
                if (m && parseFloat(m[1]) <= 10) {
                    result.rating = parseFloat(m[1]);
                    break;
                }
            }

            // íŒ”ë¡œì›Œ ìˆ˜
            const followerMatch = body.match(
                /([\\d,]+)\\s*(?:people|followers|follow)/i
            );
            if (followerMatch) {
                result.followers = parseInt(
                    followerMatch[1].replace(/,/g, '')
                );
            }

            // ìƒíƒœ
            for (const s of ['Ongoing', 'Dropped', 'Hiatus',
                             'Season End', 'Completed']) {
                if (body.includes(s)) {
                    result.status = s;
                    break;
                }
            }

            // íƒ€ì…
            if (body.includes('Manhwa')) result.type = 'Manhwa';
            else if (body.includes('Manga')) result.type = 'Manga';
            else if (body.includes('Manhua')) result.type = 'Manhua';

            // ì‘ê°€/ì•„í‹°ìŠ¤íŠ¸
            const labels = document.querySelectorAll('span, h3, dt, th');
            for (const label of labels) {
                const lt = label.textContent.trim().toLowerCase();
                const next = label.nextElementSibling;
                const nextText = next ? next.textContent.trim() : '';

                if (lt === 'author' || lt === 'author(s)') {
                    result.author = nextText || '';
                }
                if (lt === 'artist' || lt === 'artist(s)') {
                    result.artist = nextText || '';
                }
                if (lt === 'serialization') {
                    result.serialization = nextText || '';
                }
            }

            // ì¥ë¥´ (trailing comma ì œê±° + ì¤‘ë³µ ì œê±°)
            const genreLinks = document.querySelectorAll(
                'a[href*="genres"]'
            );
            const genres = Array.from(genreLinks)
                .map(a => a.textContent.trim().replace(/,$/,'').trim())
                .filter(g => g.length > 0 && g.length < 30);
            result.genres = [...new Set(genres)].join(', ');

            // ì„¤ëª…
            const descEls = document.querySelectorAll(
                'p, span.font-medium'
            );
            for (const el of descEls) {
                const text = el.textContent.trim();
                if (text.length > 100 && text.length < 5000) {
                    result.description = text;
                    break;
                }
            }

            // ì±•í„° ìˆ˜ (hrefì˜ /chapter/N ì—ì„œ ì¶”ì¶œ â€” í…ìŠ¤íŠ¸ëŠ” ì—°ê²°ë¨)
            const chapterLinks = document.querySelectorAll(
                'a[href*="/chapter/"]'
            );
            let maxChapter = 0;
            for (const cl of chapterLinks) {
                const href = cl.getAttribute('href') || '';
                const m = href.match(/\/chapter\/(\d+)/);
                if (m) {
                    const num = parseInt(m[1]);
                    if (num > maxChapter) maxChapter = num;
                }
            }
            if (maxChapter > 0) result.total_chapters = maxChapter;

            // ëŒ“ê¸€ ìˆ˜ (span ë‚´ë¶€ "449 Comments" íŒ¨í„´)
            const allSpans = document.querySelectorAll(
                'span.text-base, span.font-medium'
            );
            for (const sp of allSpans) {
                const t = sp.textContent.trim();
                const cm = t.match(/^(\d+)\s*Comment/i);
                if (cm) {
                    result.comment_count = parseInt(cm[1]);
                    break;
                }
            }

            return result;
        }""")

        return detail if detail else None

    async def _extract_comments(self, page: Page,
                                title: str) -> List[Dict]:
        """ì‘í’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ëŒ“ê¸€ ì¶”ì¶œ

        DOM êµ¬ì¡° (ê° ëŒ“ê¸€):
        div.flex
          div.flex-shrink-0.mr-3 â†’ ì•„ë°”íƒ€
          div.flex-1.min-w-0
            div.flex.items-center.gap-2.mb-1
              div.font-semibold â†’ ìœ ì €ëª…
              span.text-xs.text-zinc-400 â†’ "10 months ago"
            div.text-sm.leading-relaxed.mb-3 â†’ ë³¸ë¬¸
            ... â†’ ì¢‹ì•„ìš”, Reply ë²„íŠ¼
        """
        # ìŠ¤í¬ë¡¤ ë‹¤ìš´í•˜ì—¬ ëŒ“ê¸€ ë¡œë“œ
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.wait_for_timeout(2000)

        # "Load More Comments" í´ë¦­ (ìµœëŒ€ 5íšŒ â†’ ì•½ 150~200ê°œ)
        for _ in range(5):
            load_more = await page.query_selector(
                'button:has-text("Load More Comments")'
            )
            if not load_more:
                break
            try:
                await load_more.click()
                await page.wait_for_timeout(1500)
            except Exception:
                break

        comments = await page.evaluate("""(title) => {
            const results = [];
            const seen = new Set();

            // ëŒ“ê¸€ ë¸”ë¡: div.flex > div.flex-shrink-0 + div.flex-1 êµ¬ì¡°
            const allDivs = document.querySelectorAll(
                'div.flex-1.min-w-0'
            );

            for (const contentDiv of allDivs) {
                // ìœ ì €ëª…: div.font-semibold
                const nameEl = contentDiv.querySelector(
                    'div.font-semibold, span.font-semibold'
                );
                const name = nameEl ? nameEl.textContent.trim() : '';
                if (!name || name.length > 50 || name.length < 2) continue;

                // ì‹œê°„: span.text-xs ë˜ëŠ” "ago" íŒ¨í„´
                const timeEl = contentDiv.querySelector(
                    'span.text-xs'
                );
                let timeText = '';
                if (timeEl) {
                    const tt = timeEl.textContent.trim();
                    if (tt.match(/ago$/i)) timeText = tt;
                }

                // ë³¸ë¬¸: div.text-sm.leading-relaxed ë˜ëŠ” div.mb-3
                const bodyEl = contentDiv.querySelector(
                    'div.text-sm.leading-relaxed, div.mb-3'
                );
                let body = '';
                if (bodyEl) {
                    body = bodyEl.textContent.trim();
                }
                if (!body || body.length < 3) continue;

                // ì¤‘ë³µ ë°©ì§€
                const key = name + '|' + body.substring(0, 50);
                if (seen.has(key)) continue;
                seen.add(key);

                // ì¢‹ì•„ìš” ìˆ˜: ë²„íŠ¼ ë‚´ë¶€ ìˆ«ì (ì²« ë²ˆì§¸ = upvote)
                const buttons = contentDiv.querySelectorAll('button');
                let likes = 0;
                for (const btn of buttons) {
                    const btnText = btn.textContent.trim();
                    if (btnText.match(/^\\d+$/) && parseInt(btnText) < 100000) {
                        likes = parseInt(btnText);
                        break;  // ì²« ë²ˆì§¸ ìˆ«ì ë²„íŠ¼ = upvote
                    }
                }

                results.push({
                    reviewer_name: name,
                    body: body.substring(0, 2000),
                    reviewed_at_text: timeText,
                    likes_count: likes,
                    work_title: title,
                });
            }

            return results;
        }""", title)

        return comments or []

    # ===== ë°ì´í„° ì €ì¥ =====

    async def save_all(self, date: str):
        """ìˆ˜ì§‘í•œ ëª¨ë“  ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
        from crawler.db import (
            save_rankings, save_works_metadata, save_work_detail,
            save_reviews, backup_to_json
        )

        # 1. ë­í‚¹ ì €ì¥ (Weeklyë¥¼ ë©”ì¸ìœ¼ë¡œ)
        weekly = self.results['rankings_weekly']
        if weekly:
            save_rankings(date, self.platform_id, [
                {
                    'rank': item['rank'],
                    'title': item['title'],
                    'genre': item.get('genres', ''),
                    'url': item.get('url', ''),
                }
                for item in weekly
            ], sub_category='weekly')
            self.logger.info(f"   ğŸ’¾ Weekly ë­í‚¹: {len(weekly)}ê°œ")

        monthly = self.results['rankings_monthly']
        if monthly:
            save_rankings(date, self.platform_id, [
                {
                    'rank': item['rank'],
                    'title': item['title'],
                    'genre': item.get('genres', ''),
                    'url': item.get('url', ''),
                }
                for item in monthly
            ], sub_category='monthly')
            self.logger.info(f"   ğŸ’¾ Monthly ë­í‚¹: {len(monthly)}ê°œ")

        all_time = self.results['rankings_all']
        if all_time:
            save_rankings(date, self.platform_id, [
                {
                    'rank': item['rank'],
                    'title': item['title'],
                    'genre': item.get('genres', ''),
                    'url': item.get('url', ''),
                }
                for item in all_time
            ], sub_category='all')
            self.logger.info(f"   ğŸ’¾ All-time ë­í‚¹: {len(all_time)}ê°œ")

        # 2. ì‹œë¦¬ì¦ˆ ëª©ë¡ â†’ ì¸ê¸°ìˆœìœ¼ë¡œ rankings í…Œì´ë¸”ì—ë„ ì €ì¥
        series = self.results['series_list']
        if series:
            # ì¸ê¸°ìˆœ ì¢…í•© ë­í‚¹ìœ¼ë¡œ ì €ì¥ (ìµœëŒ€ 100ê°œ)
            save_rankings(date, self.platform_id, [
                {
                    'rank': item['rank'],
                    'title': item['title'],
                    'genre': '',
                    'url': item.get('url', ''),
                }
                for item in series[:100]
            ], sub_category='')

            # works ë©”íƒ€ë°ì´í„° ì €ì¥ (ì „ì²´)
            works_meta = [
                {
                    'title': item['title'],
                    'thumbnail_url': item.get('thumbnail_url', ''),
                    'url': item.get('url', ''),
                    'genre': '',
                    'rank': item.get('rank'),
                }
                for item in series
                if item.get('thumbnail_url')
            ]
            if works_meta:
                save_works_metadata(
                    self.platform_id, works_meta, date=date, sub_category=''
                )
            self.logger.info(
                f"   ğŸ’¾ ì‹œë¦¬ì¦ˆ ëª©ë¡: {min(len(series), 100)}ê°œ ë­í‚¹ + "
                f"{len(works_meta)}ê°œ ë©”íƒ€ë°ì´í„°"
            )

            backup_to_json(date, self.platform_id, [
                {
                    'rank': item['rank'],
                    'title': item['title'],
                    'rating': item.get('rating'),
                    'status': item.get('status'),
                    'latest_chapter': item.get('latest_chapter'),
                    'type': item.get('type'),
                    'url': item.get('url', ''),
                    'thumbnail_url': item.get('thumbnail_url', ''),
                }
                for item in series
            ])

        # 3. ìƒì„¸ ì •ë³´ ì €ì¥
        details = self.results['series_details']
        detail_count = 0
        for detail in details:
            try:
                saved = save_work_detail(self.platform_id, detail['title'], {
                    'author': detail.get('author', ''),
                    'publisher': '',
                    'label': '',
                    'tags': detail.get('genres', ''),
                    'description': detail.get('description', ''),
                    'hearts': detail.get('followers'),
                    'favorites': detail.get('followers'),
                    'rating': detail.get('rating'),
                    'review_count': detail.get('comment_count'),
                })
                if saved:
                    detail_count += 1
            except Exception as e:
                self.logger.warning(
                    f"      ìƒì„¸ ì €ì¥ ì‹¤íŒ¨ {detail.get('title', '')[:20]}: {e}"
                )
        if detail_count:
            self.logger.info(f"   ğŸ’¾ ìƒì„¸ ì •ë³´: {detail_count}ê°œ")

        # 4. ëŒ“ê¸€ ì €ì¥
        comments = self.results['comments']
        if comments:
            # ì‘í’ˆë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ì €ì¥
            from collections import defaultdict
            by_title = defaultdict(list)
            for c in comments:
                by_title[c['work_title']].append({
                    'reviewer_name': c.get('reviewer_name', ''),
                    'reviewer_info': '',
                    'body': c.get('body', ''),
                    'rating': None,
                    'likes_count': c.get('likes_count', 0),
                    'is_spoiler': False,
                    'reviewed_at': None,
                })

            total_saved = 0
            for work_title, reviews in by_title.items():
                saved = save_reviews(self.platform_id, work_title, reviews)
                total_saved += saved

            self.logger.info(f"   ğŸ’¾ ëŒ“ê¸€: {total_saved}ê°œ")
