"""
ë Œíƒ€ (Renta!) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- SSR ë°©ì‹ (HTML ì§ì ‘ ë Œë”ë§)
- IP ì œí•œ ì—†ìŒ
- img.c-contents_cover ì…€ë ‰í„°ë¡œ ì¸ë„¤ì¼ ì¶”ì¶œ
- lazyload â†’ data-srcì— ì‹¤ì œ URL
- ì¥ë¥´ë³„ ë­í‚¹: ë³„ë„ URLë¡œ ì ‘ê·¼
  - ì¢…í•©: /renta/sc/frm/page/ranking_c.htm (li.swiper-slide ê¸°ë°˜)
  - ã‚¿ãƒ†ã‚³ãƒŸ: /renta/sc/frm/search?sort=rank&span=d&site_type=t (ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class RentaAgent(CrawlerAgent):
    """ë Œíƒ€ ë§ˆì´ë„ˆì¹˜ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    # ì¥ë¥´ë³„ ë­í‚¹ ë§¤í•‘
    GENRE_RANKINGS = {
        '': {
            'name': 'ì¢…í•©',
            'url': 'https://renta.papy.co.jp/renta/sc/frm/page/ranking_c.htm',
        },
        'ã‚¿ãƒ†ã‚³ãƒŸ': {
            'name': 'íƒ€í…Œì½”ë¯¸(ì›¹íˆ°)',
            'url': 'https://renta.papy.co.jp/renta/sc/frm/search?sort=rank&span=d&site_type=t',
        },
    }

    def __init__(self):
        super().__init__(
            platform_id='renta',
            platform_name='ë Œíƒ€ (Renta!)',
            url='https://renta.papy.co.jp/renta/sc/frm/page/ranking_c.htm'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ë Œíƒ€ ì¢…í•© + ã‚¿ãƒ†ã‚³ãƒŸ ë­í‚¹ í¬ë¡¤ë§"""
        ctx = await browser.new_context(
            locale='ja-JP',
            viewport={'width': 1366, 'height': 768},
            ignore_https_errors=True,
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/120.0.0.0 Safari/537.36',
        )
        page = await ctx.new_page()
        all_rankings = []

        try:
            # ===== ì¢…í•© ë­í‚¹ =====
            genre_url = self.GENRE_RANKINGS['']['url']
            self.logger.info(f"ğŸ“± ë Œíƒ€ [ì¢…í•©] í¬ë¡¤ë§ ì¤‘... â†’ {genre_url}")

            await page.goto(genre_url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(5000)

            # DOM ê¸°ë°˜ ì¶”ì¶œ (ì¸ë„¤ì¼ í¬í•¨)
            rankings = await self._extract_ranking_page(page)

            # í´ë°±: JS evaluate
            if len(rankings) < 10:
                self.logger.info("   DOM íŒŒì‹± ë¶€ì¡±, JS evaluate ì‹œë„...")
                rankings = await self._extract_via_js(page)

            all_rankings = rankings[:100]
            self.genre_results[''] = all_rankings
            self.logger.info(f"   âœ… [ì¢…í•©]: {len(all_rankings)}ê°œ ì‘í’ˆ")

            # ===== ã‚¿ãƒ†ã‚³ãƒŸ ë­í‚¹ (ë³„ë„ URL) =====
            tatekomi_url = self.GENRE_RANKINGS['ã‚¿ãƒ†ã‚³ãƒŸ']['url']
            self.logger.info(f"ğŸ“± ë Œíƒ€ [ã‚¿ãƒ†ã‚³ãƒŸ] í¬ë¡¤ë§ ì¤‘... â†’ {tatekomi_url}")

            tatekomi_rankings = await self._extract_search_rankings(page, tatekomi_url, 'ã‚¿ãƒ†ã‚³ãƒŸ')
            self.genre_results['ã‚¿ãƒ†ã‚³ãƒŸ'] = tatekomi_rankings[:100]
            self.logger.info(f"   âœ… [ã‚¿ãƒ†ã‚³ãƒŸ]: {len(tatekomi_rankings)}ê°œ ì‘í’ˆ")

            return all_rankings

        finally:
            await page.close()
            await ctx.close()

    async def _extract_search_rankings(self, page, url: str, genre_key: str) -> List[Dict[str, Any]]:
        """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ë­í‚¹ ì¶”ì¶œ (ã‚¿ãƒ†ã‚³ãƒŸ ë“±)

        êµ¬ì¡°: .list-item_wrap > .desclist-item
        ì œëª©: .desclist-title_text
        ì¸ë„¤ì¼: .desclist-cover_link img
        URL: a href
        í˜ì´ì§€ë„¤ì´ì…˜: ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘ (ìµœëŒ€ 4í˜ì´ì§€ = 100ê°œ)
        """
        all_items = []

        for page_num in range(1, 5):  # ìµœëŒ€ 4í˜ì´ì§€ (ì•½ 25ê°œ/í˜ì´ì§€)
            page_url = f"{url}&page={page_num}" if page_num > 1 else url

            await page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(3000)

            items = await page.evaluate("""() => {
                const results = [];
                // ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ: .desclist-item ë˜ëŠ” ìœ ì‚¬ ì»¨í…Œì´ë„ˆ
                const containers = document.querySelectorAll('.desclist-item, .list-item_wrap .desclist-cover_link');

                // ë°©ë²• 1: desclist-item ê¸°ë°˜
                const descItems = document.querySelectorAll('.desclist-item');
                if (descItems.length > 0) {
                    for (const item of descItems) {
                        const titleEl = item.querySelector('.desclist-title_text, .desclist-title_link');
                        const title = titleEl ? titleEl.textContent.trim() : '';
                        if (!title || title.length < 2) continue;

                        const linkEl = item.querySelector('a[href*="/frm/item/"]');
                        const href = linkEl ? linkEl.getAttribute('href') : '';
                        const fullUrl = href ? (href.startsWith('http') ? href : 'https://renta.papy.co.jp' + href) : '';

                        const imgEl = item.querySelector('img');
                        const imgSrc = imgEl ? (imgEl.getAttribute('data-src') || imgEl.getAttribute('src') || '') : '';
                        const thumbUrl = imgSrc.startsWith('http') ? imgSrc : (imgSrc.startsWith('//') ? 'https:' + imgSrc : '');

                        results.push({ title, url: fullUrl, thumbnail_url: thumbUrl });
                    }
                }

                // ë°©ë²• 2: img.c-contents_cover ê¸°ë°˜ (í´ë°±)
                if (results.length === 0) {
                    const imgs = document.querySelectorAll('img.c-contents_cover, img[class*="cover"]');
                    for (const img of imgs) {
                        const src = img.getAttribute('data-src') || img.getAttribute('src') || '';
                        if (!src || src.includes('space.gif') || src.includes('blank') || src.includes('icon')) continue;

                        const alt = img.getAttribute('alt') || '';
                        let title = alt.replace(/ã®è¡¨ç´™$/, '').trim();
                        if (!title || title.length < 2) {
                            const container = img.closest('li') || img.closest('div') || img.parentElement;
                            const a = container ? container.querySelector('a[href*="/frm/item/"]') : null;
                            if (a) title = a.textContent.trim();
                        }
                        if (!title || title.length < 2) continue;

                        const container = img.closest('li') || img.closest('div') || img.parentElement;
                        const linkEl = container ? container.querySelector('a[href*="/frm/item/"]') : null;
                        const href = linkEl ? linkEl.getAttribute('href') : '';
                        const fullUrl = href ? (href.startsWith('http') ? href : 'https://renta.papy.co.jp' + href) : '';

                        const thumbUrl = src.startsWith('http') ? src : (src.startsWith('//') ? 'https:' + src : '');

                        results.push({ title, url: fullUrl, thumbnail_url: thumbUrl });
                    }
                }

                return results;
            }""")

            if not items:
                self.logger.info(f"   í˜ì´ì§€ {page_num}: ì‘í’ˆ ì—†ìŒ, ì¤‘ë‹¨")
                break

            start_rank = len(all_items) + 1
            for i, item in enumerate(items):
                all_items.append({
                    'rank': start_rank + i,
                    'title': item['title'],
                    'genre': genre_key,
                    'url': item.get('url', ''),
                    'thumbnail_url': item.get('thumbnail_url', ''),
                })

            self.logger.info(f"   í˜ì´ì§€ {page_num}: {len(items)}ê°œ ì¶”ì¶œ (ëˆ„ì  {len(all_items)}ê°œ)")

            if len(all_items) >= 100:
                break

        return all_items[:100]

    async def _extract_ranking_page(self, page) -> List[Dict[str, Any]]:
        """ì¢…í•© ë­í‚¹ í˜ì´ì§€ì—ì„œ DOM ì¶”ì¶œ (img.c-contents_cover ì‚¬ìš©)"""
        items = await page.evaluate("""() => {
            const results = [];
            // renta: li.swiper-slide ì•ˆì— img.c-contents_cover
            const slides = document.querySelectorAll('li.swiper-slide, li');
            let rank = 0;

            for (const li of slides) {
                const img = li.querySelector('img.c-contents_cover, img[class*="cover"]');
                if (!img) continue;

                const src = img.getAttribute('data-src') || img.getAttribute('src') || '';
                if (!src || src.includes('space.gif') || src.includes('blank')) continue;

                const alt = img.getAttribute('alt') || '';
                // alt: "ã‚¿ã‚¤ãƒˆãƒ«ã®è¡¨ç´™" â†’ ã‚¿ã‚¤ãƒˆãƒ«
                let title = alt.replace(/ã®è¡¨ç´™$/, '').trim();

                // ë§í¬ì—ì„œ íƒ€ì´í‹€ ì¶”ì¶œ ì‹œë„
                if (!title || title.length < 2) {
                    const a = li.querySelector('a[href*="/frm/item/"]');
                    if (a) title = a.textContent.trim();
                }
                if (!title || title.length < 2) continue;

                const linkEl = li.querySelector('a[href*="/frm/item/"]');
                const href = linkEl ? linkEl.getAttribute('href') : '';
                const fullUrl = href ? (href.startsWith('http') ? href : 'https://renta.papy.co.jp' + href) : 'https://renta.papy.co.jp';

                const thumbUrl = src.startsWith('http') ? src : (src.startsWith('//') ? 'https:' + src : 'https://renta.papy.co.jp' + src);

                rank++;
                if (rank <= 100) {
                    results.push({
                        rank: rank,
                        title: title,
                        url: fullUrl,
                        thumbnail_url: thumbUrl,
                    });
                }
            }
            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': '',
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    async def _extract_via_js(self, page) -> List[Dict[str, Any]]:
        """JavaScript evaluateë¡œ ì§ì ‘ ì¶”ì¶œ (í´ë°±)"""
        items = await page.evaluate("""() => {
            const results = [];
            const sections = document.querySelectorAll('ul');
            let rank = 0;

            sections.forEach(ul => {
                const lis = ul.querySelectorAll('li');
                lis.forEach(li => {
                    const a = li.querySelector('a[href*="/frm/item/"]');
                    if (!a) return;

                    const title = a.textContent.trim();
                    if (!title || title.length < 2) return;

                    const href = a.getAttribute('href') || '';
                    const img = li.querySelector('img');
                    const thumbSrc = img ? (img.getAttribute('data-src') || img.getAttribute('src') || '') : '';

                    rank++;
                    if (rank <= 100) {
                        results.push({
                            rank: rank,
                            title: title,
                            url: href.startsWith('http') ? href : 'https://renta.papy.co.jp' + href,
                            thumbnail_url: thumbSrc.startsWith('http') ? thumbSrc : (thumbSrc.startsWith('/') ? 'https://renta.papy.co.jp' + thumbSrc : ''),
                        });
                    }
                });
            });

            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': '',
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        # ì¢…í•© ë­í‚¹ ì €ì¥
        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        # ì¥ë¥´ë³„ ë­í‚¹ ì €ì¥
        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            genre_meta = [
                {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
                 'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
                for item in rankings if item.get('thumbnail_url')
            ]
            if genre_meta:
                save_works_metadata(self.platform_id, genre_meta, date=date, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
