"""
ë Œíƒ€ (Renta!) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- SSR ë°©ì‹ (HTML ì§ì ‘ ë Œë”ë§)
- IP ì œí•œ ì—†ìŒ
- ì¹´í…Œê³ ë¦¬ë³„ ëž­í‚¹ í‘œì‹œ (ì†Œë…€, ì†Œë…„, ì²­ë…„, BL, TL ë“±)
- ê° ì¹´í…Œê³ ë¦¬ ì„¹ì…˜ì—ì„œ ì•„ì´í…œ ì¶”ì¶œ
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class RentaAgent(CrawlerAgent):
    """ë Œíƒ€ ë§ˆì´ë„ˆì¹˜ ëž­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'path': '/renta/sc/frm/page/ranking_c.htm'},
    }

    def __init__(self):
        super().__init__(
            platform_id='renta',
            platform_name='ë Œíƒ€ (Renta!)',
            url='https://renta.papy.co.jp/renta/sc/frm/page/ranking_c.htm'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ë Œíƒ€ ëž­í‚¹ í¬ë¡¤ë§"""
        ctx = await browser.new_context(
            locale='ja-JP',
            viewport={'width': 1366, 'height': 768},
            ignore_https_errors=True,
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                       'AppleWebKit/537.36 (KHTML, like Gecko) '
                       'Chrome/120.0.0.0 Safari/537.36',
        )
        page = await ctx.new_page()
        all_rankings = []

        try:
            self.logger.info(f"ðŸ“± ë Œíƒ€ [ì¢…í•©] í¬ë¡¤ë§ ì¤‘... â†’ {self.url}")

            await page.goto(self.url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(5000)

            # í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ëž­í‚¹ ì¶”ì¶œ
            body_text = await page.inner_text('body')
            lines = [l.strip() for l in body_text.split('\n') if l.strip()]

            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì•„ì´í…œ ì¶”ì¶œ
            # RentaëŠ” ì¹´í…Œê³ ë¦¬ í—¤ë” + ì•„ì´í…œ ëª©ë¡ êµ¬ì¡°
            # ì•„ì´í…œì€ "è©¦ã—èª­ã¿" íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„
            rankings = []
            rank = 0

            for i, line in enumerate(lines):
                # íƒ€ì´í‹€ í›„ë³´: ì¶©ë¶„ížˆ ê¸´ í…ìŠ¤íŠ¸ (ë„¤ë¹„ê²Œì´ì…˜ ì œì™¸)
                if (len(line) >= 4 and
                    line not in ['è©¦ã—èª­ã¿', 'ç„¡æ–™ç™»éŒ²', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ã‚‚ã£ã¨è¦‹ã‚‹'] and
                    not line.startswith('ãƒ¬ãƒ³ã‚¿ãƒ«') and
                    not line.startswith('ãƒžãƒ³ã‚¬') and
                    'é…ä¿¡ä¸­' not in line):

                    # ì´ì „ ì¤„ì´ "è©¦ã—èª­ã¿"ì´ë©´ ë‹¤ìŒ ìž‘í’ˆì˜ ì‹œìž‘
                    # ë˜ëŠ” ì´ì „ ì¤„ì´ ì¹´í…Œê³ ë¦¬ í—¤ë”ì´ë©´
                    if i > 0:
                        prev = lines[i - 1] if i > 0 else ''
                        # ì¹´í…Œê³ ë¦¬ í—¤ë” íŒ¨í„´ ê°ì§€
                        categories = ['å°‘å¥³æ¼«ç”»', 'å°‘å¹´æ¼«ç”»', 'é’å¹´æ¼«ç”»', 'æ˜ åƒåŒ–ä½œå“',
                                      'ãƒœãƒ¼ã‚¤ã‚ºãƒ©ãƒ–', 'ãƒ†ã‚£ãƒ¼ãƒ³ã‚ºãƒ©ãƒ–', 'å¥³æ€§æ¼«ç”»',
                                      'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹', 'ãƒãƒ¼ãƒ¬ã‚¯ã‚¤ãƒ³', 'ã‚¿ãƒ†ã‚³ãƒŸ']

                        is_after_category = prev in categories
                        is_after_trial = prev == 'è©¦ã—èª­ã¿'

                        if is_after_category or (is_after_trial and rank > 0):
                            rank += 1
                            if rank <= 100:
                                rankings.append({
                                    'rank': rank,
                                    'title': line,
                                    'genre': '',
                                    'url': 'https://renta.papy.co.jp',
                                    'thumbnail_url': '',
                                })

            # Fallback: evaluate JSë¡œ ë§í¬ ê¸°ë°˜ ì¶”ì¶œ
            if len(rankings) < 10:
                self.logger.info("   í…ìŠ¤íŠ¸ íŒŒì‹± ë¶€ì¡±, JS evaluate ì‹œë„...")
                rankings = await self._extract_via_js(page)

            all_rankings = rankings[:100]
            self.genre_results[''] = all_rankings
            self.logger.info(f"   âœ… [ì¢…í•©]: {len(all_rankings)}ê°œ ìž‘í’ˆ")

            return all_rankings

        finally:
            await page.close()
            await ctx.close()

    async def _extract_via_js(self, page) -> List[Dict[str, Any]]:
        """JavaScript evaluateë¡œ ì§ì ‘ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            // ì¹´í…Œê³ ë¦¬ ì„¹ì…˜ë³„ ì¶”ì¶œ
            const sections = document.querySelectorAll('ul');
            let rank = 0;

            sections.forEach(ul => {
                const lis = ul.querySelectorAll('li');
                lis.forEach(li => {
                    const a = li.querySelector('a[href*="/frm/item/"]');
                    if (!a) return;

                    const title = a.textContent.trim();
                    if (!title || title.length < 2) return;

                    const href = a.getAttribute('href') || '';
                    const img = li.querySelector('img');
                    const thumbSrc = img ? (img.getAttribute('src') || '') : '';

                    rank++;
                    if (rank <= 100) {
                        results.push({
                            rank: rank,
                            title: title,
                            url: href.startsWith('http') ? href : 'https://renta.papy.co.jp' + href,
                            thumbnail_url: thumbSrc.startsWith('http') ? thumbSrc : (thumbSrc.startsWith('/') ? 'https://renta.papy.co.jp' + thumbSrc : ''),
                        });
                    }
                });
            });

            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': '',
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ëž­í‚¹ ì €ìž¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('title')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)
