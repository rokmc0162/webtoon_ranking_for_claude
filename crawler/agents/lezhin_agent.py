"""
ë ˆì§„ì½”ë¯¹ìŠ¤ (Lezhin) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- CSR ë°©ì‹ (Next.js, React ê¸°ë°˜)
- IP ì œí•œ ì—†ìŒ
- Tailwind CSS ê¸°ë°˜ ë ˆì´ì•„ì›ƒ
- div.relative.border img ì…€ë ‰í„°ë¡œ ì¸ë„¤ì¼ ì¶”ì¶œ
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class LezhinAgent(CrawlerAgent):
    """ë ˆì§„ì½”ë¯¹ìŠ¤ ì¼ê°„ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'tab': ''},
        'å°‘å¹´ãƒãƒ³ã‚¬': {'name': 'ì†Œë…„ë§Œí™”', 'tab': 'å°‘å¹´ãƒãƒ³ã‚¬'},
        'é’å¹´ãƒãƒ³ã‚¬': {'name': 'ì²­ë…„ë§Œí™”', 'tab': 'é’å¹´ãƒãƒ³ã‚¬'},
        'å°‘å¥³ãƒãƒ³ã‚¬': {'name': 'ì†Œë…€ë§Œí™”', 'tab': 'å°‘å¥³ãƒãƒ³ã‚¬'},
        'å¥³æ€§ãƒãƒ³ã‚¬': {'name': 'ì—¬ì„±ë§Œí™”', 'tab': 'å¥³æ€§ãƒãƒ³ã‚¬'},
        'BL': {'name': 'BL', 'tab': 'BLã‚³ãƒŸãƒƒã‚¯'},
        'TL': {'name': 'TL', 'tab': 'TLã‚³ãƒŸãƒƒã‚¯'},
    }

    def __init__(self):
        super().__init__(
            platform_id='lezhin',
            platform_name='ë ˆì§„ì½”ë¯¹ìŠ¤ (Lezhin)',
            url='https://lezhin.jp/ranking'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ë ˆì§„ì½”ë¯¹ìŠ¤ ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ í¬ë¡¤ë§ - API ì§ì ‘ í˜¸ì¶œ"""
        page = await browser.new_page()
        all_rankings = []

        try:
            # ë¨¼ì € í˜ì´ì§€ ë¡œë“œ (ì¿ í‚¤/ì„¸ì…˜ í™•ë³´)
            await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
            await page.wait_for_timeout(3000)

            # ì¥ë¥´ ëª©ë¡ API í˜¸ì¶œí•´ì„œ hash_id ë§¤í•‘
            genre_map = await page.evaluate("""async () => {
                try {
                    const resp = await fetch('/api/genres?type=genre_rank');
                    if (!resp.ok) return null;
                    const data = await resp.json();
                    return data?.results?.data || null;
                } catch(e) {
                    return null;
                }
            }""")

            # hash_id ë§¤í•‘ êµ¬ì„±
            hash_map = {}
            if genre_map:
                for g in genre_map:
                    hash_map[g.get('name', '')] = g.get('hash_id', '')
                self.logger.info(f"   ì¥ë¥´ API: {len(genre_map)}ê°œ ì¥ë¥´ í™•ì¸")

            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']
                tab_text = genre_info['tab']

                self.logger.info(f"ğŸ“± ë ˆì§„ì½”ë¯¹ìŠ¤ [{label}] í¬ë¡¤ë§ ì¤‘...")

                try:
                    # APIì—ì„œ í•´ë‹¹ ì¥ë¥´ì˜ hash_id ì°¾ê¸°
                    genre_hash = ''
                    if not tab_text:
                        # ì¢…í•© = ì²« ë²ˆì§¸ ì¥ë¥´
                        genre_hash = hash_map.get('ç·åˆ', '')
                    else:
                        genre_hash = hash_map.get(tab_text, '')

                    if genre_hash:
                        # API cursor paginationìœ¼ë¡œ 100ê°œ ìˆ˜ì§‘
                        rankings = await self._fetch_via_api(page, genre_hash, genre_key)
                    else:
                        self.logger.info(f"   hash_id ì—†ìŒ, ìŠ¤í¬ë¡¤ í´ë°±...")
                        rankings = await self._crawl_scroll(page, genre_key, tab_text)

                    self.genre_results[genre_key] = rankings
                    self.logger.info(f"   âœ… [{label}]: {len(rankings)}ê°œ ì‘í’ˆ")
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ [{label}] í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    self.genre_results[genre_key] = []
                    continue

                if genre_key == '':
                    all_rankings = rankings

            return all_rankings

        finally:
            await page.close()

    async def _fetch_via_api(self, page, genre_hash: str, genre_key: str) -> List[Dict[str, Any]]:
        """ë ˆì§„ API cursor paginationìœ¼ë¡œ 100ê°œ ìˆ˜ì§‘"""
        all_items = await page.evaluate("""async (args) => {
            const [genreHash, maxItems] = args;
            const results = [];
            let cursor = '';

            for (let i = 0; i < 10; i++) {
                try {
                    let url = '/api/ranking?genre_hash_id=' + genreHash + '&type=daily';
                    if (cursor) url += '&cursor=' + cursor;

                    const resp = await fetch(url);
                    if (!resp.ok) break;
                    const data = await resp.json();

                    const items = data?.results?.data || [];
                    if (items.length === 0) break;

                    for (const item of items) {
                        const comic = item.comic || {};
                        results.push({
                            rank: item.rank || (results.length + 1),
                            title: comic.name || '',
                            url: comic.id ? ('https://lezhin.jp/comic/' + comic.id) : '',
                            thumbnail_url: comic.cover_thumbnail_url || '',
                        });
                    }

                    // ë‹¤ìŒ í˜ì´ì§€ cursor (pagination.nextì— ìœ„ì¹˜)
                    const pagination = data?.results?.pagination || {};
                    cursor = pagination.next || '';
                    const hasMore = pagination.has_more_pages;
                    if (!cursor || !hasMore || results.length >= maxItems) break;
                } catch(e) {
                    break;
                }
            }
            return results;
        }""", [genre_hash, 100])

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': genre_key,
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in all_items[:100]
        ]

    async def _crawl_scroll(self, page, genre_key: str, tab_text: str) -> List[Dict[str, Any]]:
        """ìŠ¤í¬ë¡¤ ê¸°ë°˜ í´ë°± (API ì‹¤íŒ¨ ì‹œ)"""
        await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
        await page.wait_for_timeout(5000)

        if tab_text:
            try:
                tab = await page.query_selector(f'text="{tab_text}"')
                if tab:
                    await tab.click()
                    await page.wait_for_timeout(3000)
            except Exception:
                pass

        for _ in range(15):
            await page.evaluate('window.scrollBy(0, 1000)')
            await page.wait_for_timeout(800)

        body_text = await page.inner_text('body')
        rankings = self._parse_text_rankings(body_text, genre_key)

        thumb_items = await self._parse_dom_rankings(page, genre_key)
        for i, r in enumerate(rankings):
            if i < len(thumb_items) and thumb_items[i].get('thumbnail_url'):
                r['thumbnail_url'] = thumb_items[i]['thumbnail_url']
                if thumb_items[i].get('url'):
                    r['url'] = thumb_items[i]['url']

        return rankings

    async def _parse_dom_rankings(self, page, genre_key: str) -> List[Dict[str, Any]]:
        """DOMì—ì„œ ë­í‚¹ ì•„ì´í…œ + ì¸ë„¤ì¼ ì¶”ì¶œ"""
        items = await page.evaluate("""() => {
            const results = [];
            // lezhin: Tailwind CSS. ì¸ë„¤ì¼ imgëŠ” div.relative.border ì•ˆì— ìˆìŒ
            // alt="chapter_image", src=contents.lezhin.jp/... íŒ¨í„´
            // íƒ€ì´í‹€ì€ img ì—†ì´ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ URLë§Œ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘
            const imgs = document.querySelectorAll('img.w-full.h-full');
            let rank = 0;

            for (const img of imgs) {
                const src = img.getAttribute('src') || '';
                if (!src.includes('lezhin') && !src.includes('hrsm-tech')) continue;
                if (src.includes('logo') || src.includes('static/media') || src.includes('icon')) continue;

                const parent = img.closest('div.relative');
                if (!parent) continue;

                // ê°€ì¥ ê°€ê¹Œìš´ a íƒœê·¸ì—ì„œ URL ì¶”ì¶œ
                const linkEl = parent.closest('a') || parent.querySelector('a');
                const href = linkEl ? linkEl.getAttribute('href') : '';
                const fullUrl = href ? (href.startsWith('http') ? href : 'https://lezhin.jp' + href) : '';

                rank++;
                if (rank <= 100) {
                    results.push({
                        rank: rank,
                        title: '',
                        url: fullUrl,
                        thumbnail_url: src,
                    });
                }
            }
            return results;
        }""")

        return [
            {
                'rank': item['rank'],
                'title': item['title'],
                'genre': genre_key,
                'url': item.get('url', ''),
                'thumbnail_url': item.get('thumbnail_url', ''),
            }
            for item in items[:100]
        ]

    def _parse_text_rankings(self, body_text: str, genre_key: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ (N + ä½ + íƒ€ì´í‹€ íŒ¨í„´) - í´ë°±"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []

        i = 0
        while i < len(lines) and len(rankings) < 100:
            line = lines[i]
            if (line.isdigit() and 1 <= int(line) <= 100 and
                    i + 1 < len(lines) and lines[i + 1].strip() == 'ä½'):
                rank = int(line)
                if i + 2 < len(lines):
                    title = lines[i + 2].strip()
                    if len(title) >= 2:
                        genre = genre_key
                        if i + 3 < len(lines):
                            meta = lines[i + 3].strip()
                            if 'ãƒ»' in meta:
                                parts = meta.split('ãƒ»')
                                if len(parts) >= 2:
                                    genre_part = parts[-1].strip()
                                    if ' / ' in genre_part:
                                        genre = genre_part.split(' / ')[0].strip()
                                    else:
                                        genre = genre_part

                        rankings.append({
                            'rank': rank,
                            'title': title,
                            'genre': genre,
                            'url': 'https://lezhin.jp/ranking',
                            'thumbnail_url': '',
                        })
                i += 4
            else:
                i += 1

        return rankings

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('thumbnail_url')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            genre_meta = [
                {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
                 'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
                for item in rankings if item.get('thumbnail_url')
            ]
            if genre_meta:
                save_works_metadata(self.platform_id, genre_meta, date=date, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
