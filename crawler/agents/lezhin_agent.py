"""
ë ˆì§„ì½”ë¯¹ìŠ¤ (Lezhin) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- CSR ë°©ì‹ (Next.js, React ê¸°ë°˜)
- IP ì œí•œ ì—†ìŒ
- ìˆœìœ„: "N\nä½\níƒ€ì´í‹€" íŒ¨í„´
- ì¥ë¥´/ì¹´í…Œê³ ë¦¬ íƒ­ ì „í™˜ ê°€ëŠ¥
"""

import re
from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent


class LezhinAgent(CrawlerAgent):
    """ë ˆì§„ì½”ë¯¹ìŠ¤ ì¼ê°„ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    GENRE_RANKINGS = {
        '': {'name': 'ì¢…í•©', 'tab': ''},
        'å°‘å¹´ãƒãƒ³ã‚¬': {'name': 'ì†Œë…„ë§Œí™”', 'tab': 'å°‘å¹´ãƒãƒ³ã‚¬'},
        'é’å¹´ãƒãƒ³ã‚¬': {'name': 'ì²­ë…„ë§Œí™”', 'tab': 'é’å¹´ãƒãƒ³ã‚¬'},
        'å°‘å¥³ãƒãƒ³ã‚¬': {'name': 'ì†Œë…€ë§Œí™”', 'tab': 'å°‘å¥³ãƒãƒ³ã‚¬'},
        'å¥³æ€§ãƒãƒ³ã‚¬': {'name': 'ì—¬ì„±ë§Œí™”', 'tab': 'å¥³æ€§ãƒãƒ³ã‚¬'},
        'BL': {'name': 'BL', 'tab': 'BLã‚³ãƒŸãƒƒã‚¯'},
        'TL': {'name': 'TL', 'tab': 'TLã‚³ãƒŸãƒƒã‚¯'},
    }

    def __init__(self):
        super().__init__(
            platform_id='lezhin',
            platform_name='ë ˆì§„ì½”ë¯¹ìŠ¤ (Lezhin)',
            url='https://lezhin.jp/ranking'
        )
        self.genre_results = {}

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """ë ˆì§„ì½”ë¯¹ìŠ¤ ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ í¬ë¡¤ë§"""
        page = await browser.new_page()
        all_rankings = []

        try:
            for genre_key, genre_info in self.GENRE_RANKINGS.items():
                label = genre_info['name']
                tab_text = genre_info['tab']

                self.logger.info(f"ğŸ“± ë ˆì§„ì½”ë¯¹ìŠ¤ [{label}] í¬ë¡¤ë§ ì¤‘...")

                await page.goto(self.url, wait_until='domcontentloaded', timeout=20000)
                await page.wait_for_timeout(5000)

                # ì¥ë¥´ íƒ­ í´ë¦­ (ì¢…í•©ì´ ì•„ë‹Œ ê²½ìš°)
                if tab_text:
                    try:
                        tab = await page.query_selector(f'text="{tab_text}"')
                        if tab:
                            await tab.click()
                            await page.wait_for_timeout(3000)
                    except Exception:
                        pass

                # ìŠ¤í¬ë¡¤ ë‹¤ìš´ìœ¼ë¡œ lazy loading íŠ¸ë¦¬ê±°
                for _ in range(5):
                    await page.evaluate('window.scrollBy(0, 1000)')
                    await page.wait_for_timeout(500)

                # í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹±
                body_text = await page.inner_text('body')
                rankings = self._parse_text_rankings(body_text, genre_key)

                self.genre_results[genre_key] = rankings
                self.logger.info(f"   âœ… [{label}]: {len(rankings)}ê°œ ì‘í’ˆ")

                if genre_key == '':
                    all_rankings = rankings

            return all_rankings

        finally:
            await page.close()

    def _parse_text_rankings(self, body_text: str, genre_key: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë­í‚¹ ì•„ì´í…œ ì¶”ì¶œ (N + ä½ + íƒ€ì´í‹€ íŒ¨í„´)"""
        lines = [l.strip() for l in body_text.split('\n') if l.strip()]
        rankings = []

        i = 0
        while i < len(lines) and len(rankings) < 100:
            line = lines[i]
            # ìˆ«ìë§Œ ìˆëŠ” ì¤„ + ë‹¤ìŒ ì¤„ì´ "ä½"
            if (line.isdigit() and 1 <= int(line) <= 100 and
                    i + 1 < len(lines) and lines[i + 1].strip() == 'ä½'):
                rank = int(line)
                # "ä½" ë‹¤ìŒ ì¤„ = íƒ€ì´í‹€
                if i + 2 < len(lines):
                    title = lines[i + 2].strip()
                    if len(title) >= 2:
                        # ì‘ê°€/ì¥ë¥´ ì •ë³´ ì¶”ì¶œ (ë‹¤ìŒ ì¤„)
                        genre = genre_key
                        if i + 3 < len(lines):
                            meta = lines[i + 3].strip()
                            # "ì‘ê°€ / ì›ì‘ìãƒ»ì¥ë¥´ / ì¹´í…Œê³ ë¦¬" íŒ¨í„´
                            if 'ãƒ»' in meta:
                                parts = meta.split('ãƒ»')
                                if len(parts) >= 2:
                                    genre_part = parts[-1].strip()
                                    # "é’å¹´ãƒãƒ³ã‚¬ / ç·åˆ" â†’ "é’å¹´ãƒãƒ³ã‚¬"
                                    if ' / ' in genre_part:
                                        genre = genre_part.split(' / ')[0].strip()
                                    else:
                                        genre = genre_part

                        rankings.append({
                            'rank': rank,
                            'title': title,
                            'genre': genre,
                            'url': 'https://lezhin.jp/ranking',
                            'thumbnail_url': '',
                        })
                i += 4  # ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ ê±´ë„ˆë›°ê¸°
            else:
                i += 1

        return rankings

    async def save(self, date: str, data: List[Dict[str, Any]]):
        """ì¢…í•© + ì¥ë¥´ë³„ ë­í‚¹ ëª¨ë‘ ì €ì¥"""
        from crawler.db import save_rankings, backup_to_json, save_works_metadata

        save_rankings(date, self.platform_id, data, sub_category='')
        works_meta = [
            {'title': item['title'], 'thumbnail_url': item.get('thumbnail_url', ''),
             'url': item.get('url', ''), 'genre': item.get('genre', ''), 'rank': item.get('rank')}
            for item in data if item.get('title')
        ]
        if works_meta:
            save_works_metadata(self.platform_id, works_meta, date=date, sub_category='')
        backup_to_json(date, self.platform_id, data)

        for genre_key, rankings in self.genre_results.items():
            if genre_key == '':
                continue
            genre_name = self.GENRE_RANKINGS[genre_key]['name']
            save_rankings(date, self.platform_id, rankings, sub_category=genre_key)
            self.logger.info(f"   ğŸ’¾ [{genre_name}]: {len(rankings)}ê°œ ì €ì¥")
