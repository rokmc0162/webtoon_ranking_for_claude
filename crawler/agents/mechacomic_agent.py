"""
ë©”ì± ì½”ë¯¹ (ã‚

ã¡ã‚ƒã‚³ãƒŸãƒƒã‚¯) í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸

íŠ¹ì§•:
- CSR ë°©ì‹ (Playwright í•„ìˆ˜)
- í˜ì´ì§€ë„¤ì´ì…˜ 3í˜ì´ì§€ (ì•½ 50ê°œ ì‘í’ˆ)
- IP ì œí•œ ì—†ìŒ (í•œêµ­ì—ì„œë„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)

ê°œì„ ì‚¬í•­:
- wait_until='networkidle' â†’ 'domcontentloaded' (timeout ë¬¸ì œ í•´ê²°)
- ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
- ì—ì´ì „íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜
"""

from typing import List, Dict, Any
from playwright.async_api import Browser

from crawler.agents.base_agent import CrawlerAgent
from crawler.utils import get_korean_title, is_riverse_title, translate_genre


class MechacomicAgent(CrawlerAgent):
    """ë©”ì± ì½”ë¯¹ íŒë§¤ ë­í‚¹ í¬ë¡¤ëŸ¬ ì—ì´ì „íŠ¸"""

    def __init__(self):
        super().__init__(
            platform_id='mechacomic',
            platform_name='ë©”ì± ì½”ë¯¹ (íŒë§¤)',
            url='https://mechacomic.jp/sales_rankings/current'
        )

    async def crawl(self, browser: Browser) -> List[Dict[str, Any]]:
        """
        ë©”ì± ì½”ë¯¹ íŒë§¤ ë­í‚¹ 50ìœ„ í¬ë¡¤ë§

        Args:
            browser: Playwright ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤

        Returns:
            [{'rank': 1, 'title': 'ì œëª©', 'genre': 'ì¥ë¥´', 'url': 'http://...'}, ...]
        """
        page = await browser.new_page()
        rankings = []

        try:
            self.logger.info(f"ğŸ“± {self.platform_name} í¬ë¡¤ë§ ì¤‘...")
            self.logger.info(f"   URL: {self.url}")

            # 3í˜ì´ì§€ ìˆœíšŒ (ê° í˜ì´ì§€ ì•½ 17ê°œ ì‘í’ˆ)
            for page_num in range(1, 4):
                url = f'{self.url}?page={page_num}'
                self.logger.debug(f"í˜ì´ì§€ {page_num} ì ‘ì† ì¤‘...")

                # ìˆ˜ì •: wait_until='domcontentloaded' (networkidle ëŒ€ì‹ )
                # ì´ìœ : networkidleì€ ëª¨ë“  ë„¤íŠ¸ì›Œí¬ í™œë™ì´ ë©ˆì¶œ ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ì—¬ timeout ë°œìƒ
                await page.goto(url, wait_until='domcontentloaded', timeout=20000)

                # JavaScript ë Œë”ë§ ëŒ€ê¸°
                # ìˆ˜ì •: ë” êµ¬ì²´ì ì¸ selectorë¡œ ë³€ê²½ (ì‹¤ì œ DOM êµ¬ì¡°ì— ë§ê²Œ)
                # ê¸°ì¡´: '.ranking-item, .rank-item, article' (ë„ˆë¬´ ë²”ìš©ì )
                # ê°œì„ : ì‹¤ì œ ì…€ë ‰í„° í™•ì¸ í›„ ì‚¬ìš©
                try:
                    await page.wait_for_selector(
                        '.c-card, .ranking__item, .rank-item, article',
                        timeout=10000
                    )
                except Exception:
                    # Selector ëŒ€ê¸° ì‹¤íŒ¨ ì‹œ ì¶”ê°€ ëŒ€ê¸°
                    await page.wait_for_timeout(2000)

                # ì‘í’ˆ ìš”ì†Œ ì¶”ì¶œ
                items = await page.query_selector_all(
                    '.c-card, .ranking__item, .rank-item, article'
                )

                self.logger.debug(f"í˜ì´ì§€ {page_num}: {len(items)}ê°œ ìš”ì†Œ ë°œê²¬")

                for item in items:
                    try:
                        # ìˆœìœ„ ì¶”ì¶œ ("Nä½" í…ìŠ¤íŠ¸ ì°¾ê¸°)
                        rank_text = await item.inner_text()
                        rank = self._extract_rank(rank_text)

                        if not rank:
                            continue  # ìˆœìœ„ ì—†ìœ¼ë©´ ìŠ¤í‚µ

                        # ì œëª© ì¶”ì¶œ
                        title = await self._extract_title(item, rank_text)

                        if not title:
                            continue

                        # URL ì¶”ì¶œ
                        url_full = await self._extract_url(item)

                        # ì¥ë¥´ ì¶”ì¶œ (í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­)
                        genre = self._extract_genre(rank_text)

                        # í•œêµ­ì–´ ì œëª© ë° ë¦¬ë²„ìŠ¤ ì—¬ë¶€ í™•ì¸
                        title_kr = get_korean_title(title)
                        is_riverse = is_riverse_title(title)
                        genre_kr = translate_genre(genre)

                        rankings.append({
                            'rank': rank,
                            'title': title.strip(),
                            'title_kr': title_kr,
                            'genre': genre,
                            'genre_kr': genre_kr,
                            'url': url_full,
                            'is_riverse': is_riverse
                        })

                    except Exception as e:
                        self.logger.debug(f"ê°œë³„ ì‘í’ˆ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue

            # ì¤‘ë³µ ì œê±° ë° ìˆœìœ„ ì •ë ¬
            unique_rankings = self._deduplicate(rankings)

            # ìƒìœ„ 50ê°œë§Œ
            result = unique_rankings[:50]

            self.logger.info(f"   âœ… {self.platform_name}: {len(result)}ê°œ ì‘í’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
            return result

        finally:
            await page.close()

    def _extract_rank(self, text: str) -> int:
        """ìˆœìœ„ ì¶”ì¶œ ("Nä½" íŒ¨í„´ ì°¾ê¸°)"""
        if 'ä½' not in text:
            return None

        for line in text.split('\n'):
            if 'ä½' in line:
                try:
                    return int(line.replace('ä½', '').strip())
                except ValueError:
                    continue

        return None

    async def _extract_title(self, item, fallback_text: str) -> str:
        """ì œëª© ì¶”ì¶œ (selector ìš°ì„ , fallbackìœ¼ë¡œ í…ìŠ¤íŠ¸ íŒŒì‹±)"""
        # Method 1: selectorë¡œ ì°¾ê¸°
        title_elem = await item.query_selector('.title, .work-title, h3, h2')
        if title_elem:
            title = await title_elem.inner_text()
            if title:
                return title.strip()

        # Method 2: í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ ê¸´ ì¤„)
        lines = fallback_text.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 3 and 'ä½' not in line:
                return line

        return ""

    async def _extract_url(self, item) -> str:
        """URL ì¶”ì¶œ"""
        link_elem = await item.query_selector('a')
        if not link_elem:
            return ""

        url_path = await link_elem.get_attribute('href')
        if not url_path:
            return ""

        if url_path.startswith('http'):
            return url_path
        else:
            return f"https://mechacomic.jp{url_path}"

    def _extract_genre(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¥ë¥´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        genres = [
            'ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼', 'æ‹æ„›', 'ã‚¢ã‚¯ã‚·ãƒ§ãƒ³', 'ãƒ‰ãƒ©ãƒ', 'ãƒ›ãƒ©ãƒ¼', 'ãƒŸã‚¹ãƒ†ãƒªãƒ¼',
            'ã‚³ãƒ¡ãƒ‡ã‚£', 'ã‚µã‚¹ãƒšãƒ³ã‚¹', 'SF', 'å­¦åœ’', 'ã‚¹ãƒãƒ¼ãƒ„', 'ã‚°ãƒ«ãƒ¡',
            'æ—¥å¸¸', 'BL', 'TL', 'ç•°ä¸–ç•Œ', 'è»¢ç”Ÿ', 'å¾©è®', 'ãƒãƒˆãƒ«'
        ]

        for genre in genres:
            if genre in text:
                return genre

        return ""

    def _deduplicate(self, rankings: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±° ë° ìˆœìœ„ ì •ë ¬"""
        seen = set()
        unique = []

        for item in rankings:
            if item['rank'] not in seen:
                seen.add(item['rank'])
                unique.append(item)

        unique.sort(key=lambda x: x['rank'])
        return unique


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import asyncio
    from playwright.async_api import async_playwright

    async def test():
        print("=" * 60)
        print("ë©”ì± ì½”ë¯¹ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)  # ë””ë²„ê¹…ìš©

            try:
                agent = MechacomicAgent()
                result = await agent.execute(browser)

                print(f"\nâœ… Success: {result.success}")
                print(f"âœ… Count: {result.count}")

                if result.success and result.data:
                    print(f"\nìƒ˜í”Œ (1~3ìœ„):")
                    for item in result.data[:3]:
                        print(f"  {item['rank']}ìœ„: {item['title']}")
                        if item['title_kr']:
                            print(f"    í•œêµ­ì–´: {item['title_kr']}")
                        print(f"    ì¥ë¥´: {item['genre']} ({item['genre_kr']})")
                        print(f"    ë¦¬ë²„ìŠ¤: {item['is_riverse']}")
                else:
                    print(f"\nâŒ Error: {result.error}")

            finally:
                await browser.close()

        print("\n" + "=" * 60)

    asyncio.run(test())
