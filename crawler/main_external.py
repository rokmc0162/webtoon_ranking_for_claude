"""
ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ì§„ì…ì 
Usage:
    python crawler/main_external.py              # ê¸°ë³¸ (anilist + mal + youtube)
    python crawler/main_external.py --anilist    # AniListë§Œ
    python crawler/main_external.py --mal        # Jikan/MALë§Œ
    python crawler/main_external.py --youtube    # YouTubeë§Œ
    python crawler/main_external.py --trends     # Google Trendsë§Œ
    python crawler/main_external.py --reddit     # Redditë§Œ
    python crawler/main_external.py --bookwalker # BookWalkerë§Œ
    python crawler/main_external.py --pixiv      # Pixivë§Œ
    python crawler/main_external.py --amazon     # Amazon JPë§Œ
    python crawler/main_external.py --twitter    # Twitter/Xë§Œ
    python crawler/main_external.py --all        # ì „ì²´ 9ê°œ ì†ŒìŠ¤
    python crawler/main_external.py --max-works 10  # ìµœëŒ€ 10ê°œ ì‘í’ˆ
"""
import asyncio
import argparse
import logging
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crawler.db import init_db
from crawler.sns.external_db import get_works_for_external

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('crawler.external')

ALL_SOURCES = [
    'anilist', 'mal', 'youtube',
    'trends', 'reddit', 'bookwalker',
    'pixiv', 'amazon', 'twitter',
]

DEFAULT_SOURCES = ['anilist', 'mal', 'youtube']


async def run_collectors(sources: list, max_works: int = 200):
    works = get_works_for_external(max_works)
    if not works:
        logger.info("ìˆ˜ì§‘ ëŒ€ìƒ ì‘í’ˆ ì—†ìŒ")
        return

    logger.info(f"ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ìƒ: {len(works)}ê°œ ì‘í’ˆ")

    for source in sources:
        try:
            if source == 'anilist':
                from crawler.sns.anilist_collector import AnilistCollector
                collector = AnilistCollector()
                await collector.collect_all(works)

            elif source == 'mal':
                from crawler.sns.jikan_collector import JikanCollector
                collector = JikanCollector()
                await collector.collect_all(works)

            elif source == 'youtube':
                from crawler.sns.youtube_collector import YoutubeCollector
                collector = YoutubeCollector(max_titles=80)
                await collector.collect_all(works)

            elif source == 'trends':
                from crawler.sns.trends_collector import TrendsCollector
                collector = TrendsCollector()
                await collector.collect_all(works)

            elif source == 'reddit':
                from crawler.sns.reddit_collector import RedditCollector
                collector = RedditCollector()
                await collector.collect_all(works)

            elif source == 'bookwalker':
                from crawler.sns.bookwalker_collector import BookWalkerCollector
                collector = BookWalkerCollector()
                await collector.collect_all(works)

            elif source == 'pixiv':
                from crawler.sns.pixiv_collector import PixivCollector
                collector = PixivCollector()
                await collector.collect_all(works)

            elif source == 'amazon':
                from crawler.sns.amazon_collector import AmazonCollector
                collector = AmazonCollector()
                await collector.collect_all(works)

            elif source == 'twitter':
                from crawler.sns.twitter_collector import TwitterCollector
                collector = TwitterCollector()
                await collector.collect_all(works)

        except ImportError as e:
            logger.warning(f"[{source}] ì˜ì¡´ì„± ëˆ„ë½: {e}")
        except Exception as e:
            logger.error(f"[{source}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")


def main():
    parser = argparse.ArgumentParser(description='ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ê¸°')
    parser.add_argument('--anilist', action='store_true', help='AniList')
    parser.add_argument('--mal', action='store_true', help='Jikan/MAL')
    parser.add_argument('--youtube', action='store_true', help='YouTube')
    parser.add_argument('--trends', action='store_true', help='Google Trends')
    parser.add_argument('--reddit', action='store_true', help='Reddit')
    parser.add_argument('--bookwalker', action='store_true', help='BookWalker')
    parser.add_argument('--pixiv', action='store_true', help='Pixiv')
    parser.add_argument('--amazon', action='store_true', help='Amazon JP')
    parser.add_argument('--twitter', action='store_true', help='Twitter/X')
    parser.add_argument('--all', action='store_true', help='ì „ì²´ 9ê°œ ì†ŒìŠ¤')
    parser.add_argument('--max-works', type=int, default=200, help='ìµœëŒ€ ì‘í’ˆ ìˆ˜')
    args = parser.parse_args()

    try:
        init_db()

        if args.all:
            sources = ALL_SOURCES[:]
        else:
            sources = []
            for s in ALL_SOURCES:
                if getattr(args, s, False):
                    sources.append(s)

        if not sources:
            sources = DEFAULT_SOURCES[:]

        print(f"\nğŸŒ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {', '.join(sources)}\n")
        asyncio.run(run_collectors(sources, args.max_works))
        print("\nâœ… ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        sys.exit(0)

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
